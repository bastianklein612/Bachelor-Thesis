IEEE Login: "Universitaets- und Landesbibliothek Muenster"







----------------------------------------
18.05.2023

Prof. Malte Schilling(Autonomous Systems AG)
Treffen am Freitag zur Themabesprechung ?
Thema in Richtung Robotik (oder evt. Maschinelles Lernen/Reinforcement Learning)


----------------------------------------
%Bossel99&2023
Netzwerk-PW (VPN, usw.)

----------------------------------------
bis 29.05.
Suche nach vorhandenen Modellen von Hexapods in Matlab/Simscape/Simulink

----------------------------------------
29.05. 
Ein Hexapod-Model gefunden, dieses aber nur mit MATLAB 2014a ausführbar
Evt. selbst Model erstellen ?

----------------------------------------
bis 06.06.
Autoren angeschrieben, aber bisher keine Rückmeldung.
Eigenes SimScape Modell entwickelt, dieses kann momentan stabil stehen aber noch nicht laufen.

----------------------------------------
06.06.
Selbst kostruiertes Modell kann verwendet werden.
Expose-Schreiben starten.
Versuchen, Tripod-Gait zu implementieren

----------------------------------------
18.06.
Expose-Schreiben:
Titel: (?)Entwicklung eines Hexapod-Models in Simulink + RL Learning
Expose-Ansatz erstellt

----------------------------------------
20.06.
Gespäch mit Malte und Julius:
 - jetzt schon Bibliographie/ Quellen verwalten anfangen
 - Bachelorarbeit hat nun folgenden Verlauf:
	- Entwicklung des Roboters mit SimScape(physikalischen Modell)
	- Dem entstandenen Roboter das Laufen beibringen; mit inverser Kinematik die Beinbewegungen steuern
	  (sodass Bein vorbestimmtem Pfad folgt)/ Tripod gait, Wave gait, ...
	- Wenn der Roboter laufen kann, soll die Koordination der Beinbewegungen mit Hilfe von Reinforcement Learning erlernt werden
	  (d.h. welches Bein darf wann bewegt werden)
	- evt. auch die Bewegung des einzelnen Beins erlernen
 - es gibt vom Roboter in Schillings Büro anscheinend schon ein digitales Modell(dieses evt. benutzen ?)
 
 - Wieso ist das Thema interessant/ wieso wichtig daran zu arbeiten ? Welcher Nutzen kann daraus gezogen werden ?
   --> Es ist wichtig, neu entwickelte Controller schon vor dem Einsatz in einem realen Roboter in Simulationen testen zu 
       können. Beschleunigt den Entwicklungsprozess, da Änderungen direkt getestet werden können und nicht vom realen Robotersystem
	   abhängig sind. 
	   Auch schwierige Aufgaben können simuliert werden ohne wohlmöglich den realen Roboter zu beschädigen und somit den Entwicklungsprozzess
	   zurückzuwerfen.
	   Simulink ist dabei eine weit verbreitete Software für genau dieses Anwendungsgebiet, lässt modulare Entwicklung zu.
	   Alles in einem Paket.
	  
 - Tipps zum wissenschaftlichen Schreiben durchlesen
 
 
 
 
  - bis Montag erste Version des Expose an Malte und Julius schicken.
  
  
 ----------------------------------------
 24.06.
  - bis jetzt noch keine Nennung von Reinforcment Learning (in der Einleitung), wie mit einbringen ?
	-->
  
-----------------------------------------
 25.06. - 27.06.
  - Sollte man überhaupt schreiben, was man machen "könnte", oder nur was gemacht "wird" ?
	--> Hauptsächlich schreiben was gemacht "wird", wenn es noch nicht feststeht ob etwas mit in die Bachelorarbeit kommt, dann ruhig otional lassen
	
  - "z.B." oder "beispielsweise" verwenden oder immer genau nennen ?
	--> Um für Leser besser verständlich zu machen oder bei Erklärungen gerne

  - Grund für "Wieso MATLAB Simulink ?" einfügen !!!
- - - - - - - - - - - - - - - - - - - - 
  Gespräch mit Malte und Julius:
	
	- fürs Zitieren den sog. "Apa" - Style (erledigt)
	- Als Roboter-Vorlage den Roboter aus Maltes Büro: PhantomX MK4   (Modell sollte online verfügbar sein) (erledigt)
	- TITEL-IDEE: "Leveraging MATLAB Simulink for Hexapod Robot Simulation and Control" (erledigt)
	- Ziel: Abstrakter halten, eher darauf eingehen wie eine gute Lösung aussehen würde, nicht was exakt gemacht werden soll; (erledigt)
	- Zeitplan ruhig granularer aufteilen, aber in Wochenform belassen (erledigt)
	- Bisheriger Text im Ziel-Teil kann in den Lösungsansatz-Teil überführt werden (erledigt)
	- In der Problemstellung schon näher auf Thema der Bachelorarbeit eingehen, noch recht allgemein gehalten (erledigt)
	- Belege einfügen wo noch nicht vorhanden (erledigt)
	
	- 'Verwandte Arbeiten' ausformulieren
	
-----------------------------------------
28.06 - 30.06
 - Installation von ROS2 zum entpacken einer urdf.xacro-Datei nach .urdf 
	- Installation macht Probleme, schlechte Anleitung bzw. nicht up-to-date
		--> Installation nicht weiter verfolgt, anderes Modell von PhantomX als .urdf gefunden
		
-----------------------------------------
01.07. - 03.07.
 - Anpassungen aus letztem Gespräch umgesetzt
 
----------------------------------------- 
04.07.
 - "URDF Primer" von MathWorks lesen (erledigt)
 - PhantomX-Modell dazu bringen, seine Beine an bestimmte Positionen zu bewegen(InverseKinematics) (mit IK-Tool schon möglich, in Simulink noch nicht)

- Gespräch mit Malte und Julius:
		- Ziel zu konkret, eher darauf eingehen was Modell können soll(Lern-Algorithmen sollen anwendbar sein, etc. --> man soll drauf aufbauen können)  (erledigt)
		- Ins Englische übersetzen und in Englisch weiterschreiben (erledigt)
		- Related works ausbauen (erledigt)
		- Expose nicht länger machen, eher schon zu lang (erledigt)
		- Darauf eingehen, dass ein vielfältiges Tool mit vielen Möglichkeiten gesucht wurde (erledigt)
		- Modell welches ich mit Simulink entwickle, soll die Grundlage bilden um darauf aufbauend weiter zu forschen (erledigt)
		- Nicht "Ich soll das verwenden, deswegen mach ich das" ! (erledigt)
		- Wir wollen ein bestimmtes Ziel erreichen, nicht 'UMBEDINGT' MATLAB verwenden und damit was machen (fängt bei Kapitel Ziel an) (erledigt)
		- ab Zielteil im 'Aktiv Präsenz' schreiben (erledigt)
		- Lösungsansatz abstrakter, nicht so konkret (erledigt)
		
		
		- Tabelle oder timechart im Zeitplan verwenden ?
		- homogene Transformationen ansehen(Foto und VL./Internet)
		- Do. (ab 13.07) 10:15 Uhr Oberseminar
		
----------------------------------------- 
04.07 - 12.07. 
	- Umsetzung der Anmerkungen aus dem Gespräch

----------------------------------------- 
12.07. - 19.07.
	
	- 13.07.: Oberseminar zu viert; Malte und Julius nicht anwesend, da in münd. Prüfungen

	- Überarbeitungen am Modell und Expose
	- Gespräch mit Malte am 19.07.:
		- Expose noch nicht angesehen
		- iterative inverse Kinematik, welche Simulink zur Verfügung stellt, relativ langsam --> evt. selbst analytischen Solver implementieren
		- Anfangen, Thesis zu schreiben
		- Aufbau:
			-Introduction
			-Background/ Related Works  --> Hiermit anfangen, da andere Abschnitte sich noch stark ändern könnten
			-Methods
			-Results
			-Discussion/Conclusion
		
		- Bachelorarbeit kann angemeldet werden
	
----------------------------------------- 
20.07. - 25.07.
	- Grundstruktur der Thesis aufgestellt
	- Einfachen Tripod-gait erzeugt, Bewegungsablauf noch sehr starr festgelegt
	- selbst implementierte analytische IK funktioniert(bis jetzt nur an einem Bein, sollte aber übertragbar sein)
	- Malte mit Feedback zum Expose: Etwas Nachschärfen im Objective nötig, momentan sehr darauf beschränkt eine gute Simulation zu erzeugen. 
	  Fragestellung etwas öffnen, darauf beziehen was uns dies ermöglicht(z.B. Lernen) 
	
----------------------------------------- 
26.07 - 02.08.
	- Feedback zum Expose umgesetzt und an Malte weitergeleitet
	- selbst implementierte analytische IK funktioniert und lässt das Modell wesentlich schneller laufen
	- Ellipse-/Pattern-Generator mehrfach umgebaut (besteht nun aus eliptischer Swing-Phase und einer linearen Bewegung auf dem Boden in der Stance-Phase
	- durch Umbau des Pattern-Generators ist nun folgendes möglich:
		- Frequenz der Gesamtbewegung kann angepasst werden(Swing+Stance zusammen)
		- 'Duty-Cycle' der Swing-Phase kann prozentual angegeben werden (z.B. 50% --> Swing und Stance gleich lang)
		- 'Offset' eingeführt; beim Start der Simulation dürfen nicht alle Beine gleichzeitig die selbe Bewegung starten, deshalb Verzögerung der Abläufe möglich
	
	- Durch Umbau können nun auch andere Gaits dargestellt werden, so funktionieren nun Wave-, Tetrapod- und Tripod-Gait
	- Problem des Ruckelns beim Laufen besteht noch immer, möchte ich durch Einführung eines Höhen-Controllers beheben
	 (Roboter sackt beim Bewegen immer leicht ab und neu gesetzte Beine drücken ihn dann ruckartig hoch)
	 
----------------------------------------- 
03.08.
	- Oberseminar:
		- Zwischenvortrag von Florian Warmeling(...Adaptive und intelligente Systeme in Simulink: Fallstudie mit ABS)
		- Habe kurz mein Thema vorgestellt; Paula hat vorgeschlagen, dass ich mein Modell mit dem von Sanando kompatibel machen sollte
		- Mit Sanando 1/2 h über sein Projekt unterhalten
		
		- Mitgenommene Ideen:
			- Beim Vortrag eine visuelle Übersicht des gesamten Systems mit einbringen; Paula hat es als Beispiel als Diagramm aus versch. Blöcken dargestellt
			  (Was baut worauf auf ? / Wo werden Daten übergeben/ weitergeleitet ? / etc.)
			- Vortrag gut strukturieren
			- Auch auf aktuellen Stand eingehen
			- Simulink-Modell nur soweit erklären wie nötig
			- Ausblick in Zukunft geben
			- Genau definieren, was der eigene Beitrag ist
			
	- Weiterarbeit am Höhencontroller
-----------------------------------------
04.08.
	- Beginn des Schreibens im Background-Teil
	- Paper gelesen(...revealsFunctionalRoleOfEmbodiedSensimotor...)
	- Sanandos Modell als .zip erhalten
	
-----------------------------------------
05.08. - 08.08.
	- Höhencontroller entfernt, neu tunen des PID-Controllers hat ausgereicht(der hat einfach zu langsam reagiert)
	- Im Background-Teil einige Absätze geschrieben
	- Versucht, die Bewegung in alle Richtungen zu ermöglichen
	  Rotation des generierten Pfads funktioniert soweit auch, aber IK-Solver ist noch mangelhaft
	  
	  --> Rework des gesamten Leg Movement Generators nötig, zu unübersichtlich
	  --> IK-Solver muss auch überarbeitet werden
	  --> Sub-Routine, welche den Roboter zu Beginn in Startposition bringt, muss auch erstellt werden
	  
-----------------------------------------
09.08.
	- Angefangen eine eigene Library mit oft verwendeten Blöcken zu erstellen
	- Leg Movement Generator überarbeitet, Rotation des Bewegungsablaufs funktioniert nun --> IK muss nicht reworked werden
	  Problem war, dass Rotation an falscher Stelle durchgeführt worden ist (zu spät, nachdem offsets schon drauf)
	
	- x,y,z-offsets zusammengefasst

-----------------------------------------
10.08.
	- Zwischenvortrag Marcel Schiller(schwer zu verstehen, keine Einleitung und sehr viel Inhalt ohne viele Grafiken; Genetic Algorithms Literature Study)
	- Prof. Paula Herber persönlich getroffen
	- eigener Zwischenvortrag mit Paula abgesprochen am 24.08 oder 31.08. geplant
	- Zugang zu Gitlab von der ES-Gruppe erhalten
	
-----------------------------------------
11.08. - 13.08.
	- MK3 Videos aufgezeichnet
	- RL-Toolbox einigermaßen verstanden, mit dieser experimentiert
	- Model umgebaut
	
	--> Festgestellt, dass die momentane Steuerung für RL eher weniger gut geeignet ist
	
-----------------------------------------	
14.08. - 15.08.
	- weiter mit RL experimentiert
	- erstes Mal geschafft, RL ohne Error über mehrere Stunden laufen zu lassen; Ergebnis ok, aber nicht gut
	
	
-----------------------------------------
16.08. 
	- Background fortgeführt(Hexapod, Simulink, Simscape)
-----------------------------------------
17.08.

	- Oberseminar:
		- Zwischenvortrag am 31.08. ! (2-3 Tage vorher an Malte, falls noch besprechen/ für Paula spätestens zu Beginn des Vortrags 
		- Modell so umbauen, das Input angibt, wann Swing-Phase initiiert werden soll ? (Malte)
		- Rel. Works muss nicht groß sein (Abgrenzung von den 2-3 wichtigsten Arbeiten)
		- Textgröße von Grafiken muss groß sein(Vortrag/Thesis); am besten so groß wie umgebender Text
		- math. Formeln statt Code zeigen
		- Background kann ruhig detailierter
		- Swing-/Stance-Cycle mit im Background-Teil erklären
		- so schreiben, dass jemand ohne gleichen Schwerpunkt folgen kann(also muss nicht von anderem Studiengang verstanden werden!)
		- Redundanzen sind ok, aber nicht immer von 0 anfangen sondern wieder aufgreifen
		- Vortrag anfangen ! (Vorgaben im Learnweb lesen)
		- Bis nächste Woche Rev. des Vortrags erstellen

	- Zwischenvortrag begonnen; zumindest schonmal Struktur
	- Fortschritt im Background-Teil(PID-Controller)

-----------------------------------------
18.08.
	- RL-Learning Grundlagen im Background begonnen 
	- Einarbeiten in RL; Gute Quellen von Malte erhalten		

-----------------------------------------
19.08. - 20.08.
	- RL-Blogs gelesen
	- RL-Abschnitt weitergeschrieben

-----------------------------------------
21.08.
	- 1. VL Deepmind RL Introduction(David Siver)
	sonst nichts

-----------------------------------------
22.08.
	- Weiter an RL Abschnitt geschrieben
	
-----------------------------------------
23.08.
	- Grafiken zur Beschreibung des Modells begonnen
	- etwas Inhalt in Vortrag
	
-----------------------------------------
24.08.
	- Oberseminar:
		- Diagramme kamen recht gut an; Pfeil zwischen 'Environment' und 'Physical Hexapod' fand Paula verwirrend --> verbesseren
		  Nochmal genau schauen, ob Diagramme wirklich das Modell repräsentieren !
		- Paula hat vorgeschlagen eine noch übersichtlichere Grafik für die Einleitung zu erstellen --> Worum geht es von ganz weit außen draufgeschaut
		- Bachelorarbeit kann komplett ohne schriftliche Formulare online angemeldet werden ! --> diese Woche machen !
		- Titel der Arbeit nochmal genau ansehen; kann man den noch verbessern ?
		
	- Background weitergeschrieben
	
-----------------------------------------
25.08.
	- Sanandos Modell angeschaut
	- 2. RL Lecture
	
-----------------------------------------
26.08.
	- TODOs vor Zwischenvortrag:
		- bis Sonntag-Mittag(27.08.) 1. Rev. des Vortrags erstellen
		- Simulink Modell etwas auffrischen, damit so übersichtlich wie möglich
		- für Vortrag alle Diagramme und Bilder erstellen
		- Vortrag üben !!!
		
		
	- Simulink-Modell als SVG: handle=get_param('Sandbox','handle');
							   print(handle,'-dsvg','Sandbox');
							   winopen Sandbox.svg;
	
-----------------------------------------
27.08.
	- Zwischenvortrag weiter vorbereitet, Grundlagen so gut wie fertig
	- Bilder des Modells aufgenommen
	- Grafik für IK und PID erstellt
	
-----------------------------------------
28.08.
	- Titelideen:
		- Leveraging MATLAB Simulink for Hexapod Robotics: Simulation, Control and Learning				<----- Winner
		- Leveraging MATLAB Simulink for Hexapod Robot Simulation, Control and Reinforcement Learning
		- Leveraging MATLAB Simulink for Hexapod Robot Simulation, Control and Learning
		- A MATLAB Simulink based approach to Hexapod Robot Simulation and control
		- A MATLAB Simulink based approach to Hexapod Robot Simulation, Control and Learning
		
		- Projektübersichts-Grafik erstellt
		- Weiterarbeit an Vortrag
		- Vortrag geübt
		
-----------------------------------------
29.08.
		- PROBLEM mit Signal Generator aufgefallen(1 Hz, 50% duty, 0 offset;  x_amplitude = 0.07, y_amplitude = 0.04) 
		  --> Nach Zwischenvortrag auf jeden Fall testen !!!
	
		- Vortrag großteilig fertig gestellt, RL und Hauptteil benötigen noch etwas Arbeit(+ Quellen)

-----------------------------------------
30.08.
		- Zwischenvortrag finalisiert und an Malte, Paula und Julius geschickt
		- Vortragen geübt

-----------------------------------------
31.08.
		- Zwischenvortrag gehalten:
			- gut vorgetragen, Tempo und Erklärungen gut verständlich
			- Roter Faden fehlt 
				--> Übersicht öfter verwenden um zu zeigen, wo man sich befindet
				--> Schon zu Beginn damit anfangen zu sagen, warum man etwas erklärt(z.B. PID, wofür ?)
			- Hervorheben, was der eigene Beitrag ist !!!
			- Quellen für Text mit auf jew. Folie packen (als Zahl die auf Ende verweist)
			- Simulink-Modell Übersichtsgrafik erweitern, kann gut als Übersicht über das gesamte Projekt genutzt werden
			  Wie Paula schon gezeigt hat, Einflüsse von außen mit aufnehmen(urdf-import,... / RL mehr zeigen, kleiner aufteilen)
			  
			  
		- Bachelorarbeit online angemeldet
-----------------------------------------
01.09.
		- Entwicklung von neuem Swing-Stance-Cotroller begonnen
		
-----------------------------------------
02.09.
		- Neuer Swing-Initialisation Ansatz für den Controller funktioniert
		- Feste Gaits auf neuen Controller angepasst
		- RL Learning mit angepasstem Controller getestet
		
-----------------------------------------
03.09.	

		- RL für 8h(ca. 250 Ep.) laufen lassen, bisher keine guten Ergebnisse --> Länger laufen lassen 
		
-----------------------------------------
04.09.
		- RL laufen gelassen
		- Bestätigung für Anmeldung der Bachelorarbeit erhalten
		- 3./4. RL Lecture

-----------------------------------------
05.09.
		- Oberseminar: 
			- Abschluss-Vortrag von Morten(Übertragung von SystemC nach FramaC zur deduktiven Verifikation(?); am Beispiel eines ABS)
			- Abschluss-Vortrag von Sanando (Abstract Hexapod...)
				- Gait wird nur durch Stateflow-Chart definiert; evt. anpassbar über Swing-Phase-Anteil
				- Ob Hexapod geradeaus läuft mMn. nicht sinnvoll definiert(?) (Mittelwert der Bein-Geschwindigkeiten auf einer Seite als v auf der Seite...)
				- Stateflow gibt als outputs: Start-Position der Swingphase[x], Mode[Swing/Stance] und Stop-Signal[wenn Bein im PEP warten muss]
			- Malte auf Reward-Definition angesprochen; will mir Paper schicken

		- zurückgelegte Strecke in Reward-Funktion vergessen.... --> Deswegen keine guten RL-Ergebnisse...
			--> Reward neu definieren

		- Neuen reward definiert, abhängig von x, y, und Höhenunterschied (immer noch temporär, da Malte noch Paper schicken wollte)
		- ca. 400 Ep. trainiert, Roboter steht bis jetzt nur(evt. hat Höhenveränderung zu großen Einfluss auf reward und x zu wenig)
		
-----------------------------------------
06.09.
		- Training weitergeführt, bis jetzt immer noch keine großen Fortschritte
		- Paper von Malte erhalten --> Festgestellt, dass ich x-Position statt Geschwindigkeit in x-Richtung im Reward habe
									   --> Reward auf Geschwi. umgestellt

-----------------------------------------
07.09.		

		- RL mit neuem Reward gestartet
		- Hauptteil der Bachelorarbeit angefangen(Hexapod-Modell)
		- RL im Background weitergeschrieben
		
-----------------------------------------
08.09.		
		- RL Training (kein Ergebnis)
		- Schreiben an Hauptteil und Background
		
-----------------------------------------
09.09.
		- RL Training(2000 Ep. erledigt, kein Ergebnis)
		- Simulationsgeschwindigkeit erhöht, step-size konnte noch vergrößert werden
		- Schreiben an Hauptteil und Background
		
-----------------------------------------
10.09.
		- RL Training(kein Ergebnis)
		- Schreiben an Hauptteil und Background; einige Grafiken schonmal eingefügt
		
-----------------------------------------
11.09.	
		- "RL learns best when inputs are normalized between N(0,1)"; also normalverteilt um 0 mit Standardabweichung 1
		   --> morgen anpassen  X
		- Weiter am Hauptteil geschrieben; Bilder und Tabellen eingefügt
		
-----------------------------------------
12.09.	
		- Bei RL Agent nach knapp 1900 Ep. immer noch kein steigender Reward
			--> evt. Averaging Window zu klein ?
			--> DDPG braucht allerdings auch lange zum trainieren(aber nicht SO lange (?))
			
		- Fortschritt im Hauptteil
			- Abschitt über Hexapod-Modell erweitert
			- Bilder + Tabellen eingefügt
			
-----------------------------------------
13.09.
		- RL Learning neu gestartet mit größerem Averaging Window; PPO und DDPG ausprobiert
		
-----------------------------------------
14.09.
		- nicht bei Oberseminar gewesen
		- PPO(16) Agent nochmals gestartet(PC gestern abgeschaltet, nicht gespeichert)
		- Weiterschreiben am Hauptteil
			- Environment
			- Stichpunkte in RL-Abschnitt
		
-----------------------------------------
15.09.
		- PPO(16) reward hat geflatlined
		- DDPG(16) gestartet
		--> Falls Flatlining des Rewards weiterhin Problem ist: Ankunft eines Beines am PEP als eigene, diskrete Observation an agent liefern ?

-----------------------------------------
16.09.
		- mehrere Agents laufen lassen, mit keinem zufrieden
		- wenig Fortschritt beim Schreiben
		--> Parallelisierung um Lernfortschritt schneller voranzutreiben (?)
		
-----------------------------------------
17.09.
		- RL-Probleme bestehen weiterhin, hie Zusammenfassung aller Ideen:
			--> Lernen parallelisieren, mehr Episoden
			--> Anstatt kontinuierlichem action space diskreten verwenden
			--> Ankunft eines Beines am PEP als diskrete Observation an agent liefern
			
		
		- RL-Skript komplett umgeschrieben um Parallelisierung zu ermöglichen
			--> Parallelisierung erfolgreich, nun 8 Agents parallel(ca. 1000 Ep./h bei 0.5ms step-size)
		- über 9000 Episoden mit DDPG_16 Agent laufen lassen, kein Ergebnis aber prove-of-concept für Parallelisierung
		
		- für jeden Agententyp eine eigenes Erzeugungs-Skript schreiben(für DDPG und schon erfolgt)
		

-----------------------------------------
18.09.	
		 - Erzeugungs-Skript für PPO_continuous geschrieben
		 - PPO-Training:
			- flatlining 500 Ep.
			
		- Internetausfall, kein weiteres Training
		
-----------------------------------------
19.09.			
		- Aufgefallen, das Reward-Signal ohne Distanz definiert ist(wahrscheinlich nicht gespeichert); korrigiert
		
		- DDPG_128 gestartet:
			- mehr als 6000 Ep.
			- Review der gut performenden Agenten morgen
			
		- Bisherige Arbeit durchgelesen und Korrekturen vorgenommen
		- auch neue Dinge hizugefügt
		
-----------------------------------------
20.09.		
		- Zusammenfassung der letzte 2 Wochen:
			- Beschreibung des Hexapod-Modells und Controllers im Hauptteil
			
			- RL-Abschnitt noch nicht weit ausgeführt, da noch nicht sicher was am Ende funktionieren wird
			
			- Wenig Erfolg mit RL soweit, Vorwärtsbewegung ja, aber mit Schleifen der Beine und nicht konstant
				- (PPO agent stark zuckend, deswegen schwierig mit diesem zu lernen)
				
			- RL parallelisiert, um mehr Episoden durchführen zu können(8 Agenten parallel)
			
			
				--> Modell und Controller soweit abgeschlossen, nur noch RL als Problem
				--> viel Fortschritt beim Schreiben, weniger Fortschritt beim RL
			
			
			--> Erste vollständige Version der Arbeit hoffentlich in 1W. , in 3W. dann fertig sein
			

	
		- Oberseminar:
			- Reward Signal umschreiben, Instabilität geringer bestrafen ?  und Schleifen von Beinen bestrafen ?
			- mehr inputs an agenten
			- in 1 Woche Arbeit an Malte zum testlesen schicken
			- Termin für Vortrag überlegen(Paula 2W in Okt weg) tpx1/ Vortrag muss eingetragen werden 
			
			
		- IK Solver beschreiben, wie funtioniert er und welche vorteile hat er
		
		- dynamische Reibung stark erhöht um Nachschleifen der Beine zu verhindern
		
-----------------------------------------
21.09.	
		- RL-Abschnitt im Hauptteil weitergeschrieben
		
-----------------------------------------
22.09.	
		- PPO_continuous gestartet --> 
		- Eigene mit anderer Arbeit verglichen
		- Conclusion/ Outlook Stichpunktliste  begonnen
		- Überarbeitung Introduction-Kapitel begonnen 
		
		--> Spezifischer RL Algorithmus muss beschrieben werden ! (DDPG, PPO, etc. ...)

-----------------------------------------
23.09.	
		- In den letzten Tagen 0 Fortschritt beim RL
		- heute minimale Änderungen am Dokument
		
-----------------------------------------
24.09.		
		- PPO agent ca. 2000 Ep. laufen gelassen, kein Erfolg
		
-----------------------------------------
25.09.				
		- Fragen bezüglich RL auf MATLAB- und Stackoverflow-Website gestellt
		- Änderungen am DDPG agent vorgenommen, training gestartet
		
		- Ziele beim Schreiben:
			- IK Solver beschreiben, geo. Zeichnung als Grafik anfertigen  --> erledigt
			- Introduction überarbeiten 
			- Zitieren
		
		- DDPG mit geänderten Parametern laufen gelassen, kein Erfolg
		
-----------------------------------------
26.09.				
		- Observation-Space um alpha-Beschleunigungen erweitert
		- Reward beteht nun nur noch aus x-/ y-Geschwindigkeit und Höhenunterschied
		- DDPG gestartet(Layeränderungen zurückgesetzt; bis auf clippedReLU)
			--> Festgelegt, min. 5000 Ep. laufen zu lassen !!!  --> nicht eingehalten (ca. 3000 Ep.)
			
		- vielversprechenden DDPG-Lauf mit sehr geringer Learnrate aus neuem Paper gestartet
			
		
-----------------------------------------
27.09.		
		- DDPG-Lauf weitergeführt
		- Analytischen IK-Solver beschrieben und Grafiken angefertigt
		
-----------------------------------------
28.09.	
		- nicht an Oberseminar teilgenommen
		- IK-Solver-Grafiken und -Text fertiggestellt
		- nach über 10000 Ep. DDPG zwar vielversprechender Verlauf, aber kein Reward-Trend nach oben;
		  Kein Flatlining, viel Variation mit zwischenzeitlichen Hoch-und Tiefpunkten
			--> Weiter tunen, sieht besser aus als alles vorher
	
		- Einige kleine Stellen im Hauptteil überarbeitet
		- Abstract erweitert
	
-----------------------------------------
29.09.		
		
	- DDPG_12000 trainingStats verloren durch herunterfahren
	- Neu gestartet mit einer Änderung: StandardDeviation: 0.1 --> 0.05;
	
	- RL-Teil im Background umgeschrieben
	
-----------------------------------------
30.09.		
	- rlMultiAgentTrainingOpts anstatt rlTrainingOpts  --> Nicht umgesetzt
	- stepsUntilDataIsSent wird nicht empfohlen zu verwenden  --> umgesetzt
	- 1. Rewrite der Arbeit gestartet, Belege eingefügt
	
-----------------------------------------
01.10.		
	- viele Belege im Background hinzugefügt
	- DDPG neu gestartet, da MATLAB Parallel Workers dauernd abschmierten
	
-----------------------------------------
02.10.
	- Rewrite fortgeführt
	- Desktop über Nacht im Wohnzimmer weiterlaufen lassen um mehr Rechenzeit zu erhalten

-----------------------------------------
03.10.
	- Desktop ab jetzt immer Nachts im Wohnzimmer, ca. 6000-8000 Ep. möglich
	- Rewrite
	
-----------------------------------------
04.10.	
	- Oberseminar verpennt
	- Rewrite im Background bis 'Insect Locomotion fertig'
	
-----------------------------------------
05.10.
	- Rewrite fortgeführt mit Inverse Kinematic
	- DQN agent ausprobieren, dieser hat diskreten action space 
	
-----------------------------------------
06.10.	
	-Rewrite, Inverse Kinematic fertig
	- 10000 Ep. DDPG Training ohne Erfolg, dann durch herunterfahren verloren
	
-----------------------------------------
07.10.	
	- diskreten actionSpace und DQN-Agent erstellt
	- Rewrite vom Background fertig gestellt
	- Rewrite von Related Works so gut wie fertig
	
-----------------------------------------
08.10.
	- Rewrite von Related Works fertig
	- Rewrite von Hauptteil begonnen
	- Diskreter DQN-Agent hat über Nacht nicht gut performed
	- diskreten PPO-Agenten aufgesetzt, ebenfalls keine überzeugende Performance
	- continuous DDPG aufgesetzt über Nacht
	
-----------------------------------------
09.10.
	- ERSTES MAL LERN-ERFOLG zu verzeichnen, jedoch durch reward-function exploit --> Springendes Verhalten
	- Anpassen der Reward-Funktion, aber bisher ohne Erfolg
	- über Nacht DDPG aufgesetzt
	
-----------------------------------------
10.10.
	- DDPG ohne Erfolg
	- Reward weiter angepasst
	- isDone Subsystem eigeführt um falsches Verhalten direkt zu bestrafen und Episode zu beenden
		--> Vielversprechende erste Versuche, aber Anpassungen nötig
	- Fortführung vom Refactor im Hauptteil:
		- Approach-Section geschieben
		- Hexapod-Modell Description begonnen
	
	- Ideen für RL morgen:
		- Reibung wieder erhöhen ? (bis jetzt dort eig. keinen exploit gesehen)
		- diskreten action space agent mit der neuen Reward-Funktion und isDone testen --> Definitiv testen
		- Layer im actor und critic hinzufügen ? (oder eher entfernen ??)
		- y-Distanz evt. erst mal gar nicht oder weniger stark bestrafen ?
		
	- über Nacht DDPG mit neuem reward und isDone aufgesetzt
	
-----------------------------------------
11.10.
	- DDPG flatlining		
	- Reward-Funktion vereinfacht, nur noch x-/z-Speed und Höhe  
		--> Nochmal nachgebessert und x-Speed wesentlich höher beloht, um stehen bleiben zu verhindern
		    --> Vielversprechender erster DDPG-Lauf
	
	- Refactor im Hauptteil:
		- Hexapod-Modell Description fertig gestellt
		- Einleitenden Absatz zur Beschreibung des Simulink-Modells hinzugefügt

-----------------------------------------
12.10.
	- Refactor im Hauptteil:
		- Environment Description im Hauptteil fertiggestellt
		
	- RL-Fortschritt:
		- weiteren Reward-Exploit festgestellt
		- Anpassungen vorgenommen, neue Sensordaten für die Höhe jedes Beins hinzugefügt
		-PPO-Agenten mit großem NN(100) aufgesetzt
		
-----------------------------------------
13.10.
	- DDPG-Agenten mit großem NN(100) aufgesetzt
	
-----------------------------------------
14.10.
	- große NN jetzt Standard 
	- Reward nochmals groß umgeschrieben, Energieverbrauch wird mit einbezogen um  Effizienz zu belohnen
	- Training ohne Ergebnisse
	- langsamer Fortschritt beim Refactor
	
-----------------------------------------
15.10.
	- Fehler in Energieverbrauch gefunden --> Rohe Drehmomentwerte anstatt 'limitierte' verwendet, dadurch viel zu hohe Werte
	- RL hat auch heute keinen guten Eindruck gemacht, aber Testen geht weiter
	- Liste mit getesteten RL-Setups/ Parametern erstellt um evt. Pattern zu finden
	- Refactor im Hauptteil bis auf IK Sover und RL fertig
	
-----------------------------------------
16.10.
	- RL über Nacht nicht erfolgreich
	- RL-Test über Tag ebenfalls flatlining
	-Refactor-Fortschritt:
		- Analytischen Ik abgeschlossen
		- Signal Generator erweitert und umgeschrieben
		-
	
	- Morgen(Dienstag) erste Version an Malte schicken
	
	
	
-----------------------------------------
17.10.
	- 02:05 Uhr: DDPG-Agent mit 120 Reward zeigt sehr vielversprechendes Verhalten im Test !!!
					- bewegt sich nach vorn
					- wenige Zuckungen
					- noch kein richtiger Gait, eher ein nach vorn schleifen
	
		--> Reward scheint gut definiert zu sein
		
	- DDPG-Agent hat max. 146 reward errreicht und zeigt erste anzeichen eines richtigen gaits
	- Da Agent nicht auf einen hohen reward konvergiert sondern immer wieder abstürzt, Training nach ca. 8000 Ep. abgebrochen und gesichert
	- Neuen DDPG-Agenten mit leicht abgeänderten Parametern aufgesetzt (actor-LR:1e-4 --> 1e-3 / Standard-Deviation: 0.05 --> 0.1)
		--> Erzielt nach > 4000 Episoden schlechteres Ergebnis(<120)
	
	- DDPG ohne termination und termination-punishment als nächtes aufgesetzt
		--> es fällt schnell auf, dass Episodenterminierung sinnvoll ist um Stillstand zu vermeiden und Training zu beschleunigen
			--> Abgebrochen
	- diskreten PPO aufgesetzt (mit Terminierung)
	
	
-----------------------------------------
18.10.
	- Ergebnis des PPO-Nachtlaufs:
		- kommt über einen reward von -50 im Durchschnitt nicht hinaus, max. Reward < 40
		- Training nach 5000 Ep. abgebrochen und neuen DDPG-Versuch gestartet
	
	
	- Oberseminar:
		- beim RL sehr viel im Kreis gedreht, viele Variablen an denen man drehen kann
		- erster Lernerfolg nutzt reward-funktion aus, mehr springen als laufen
		- mittlerweile wirklich erster Erfolg mit neu definiertem reward (Energiebedarf und Terminierung)
		- beim Schreiben:
			- erste Version gestern an Malte geschickt
			- Teil in dem es um RL geht muss noch ausgeführt werden, da momentan noch zu viele Änderungen
		
		- Hexapod-Modell in gitlab hochladen
		- 08.11. Abschlussvortrag ?
		
-----------------------------------------
18.10.		

	DDPG-Agent von gestern und über Nacht(> 10000 Ep.):
		--> > 140 reward, weiter laufen lassen
	
	- TODO:
		- Malte fragen, ob 08.11. als Abschlussvortrags-Termin passen würde
		- Hexapod-Modell separieren und für Julius in Gitlab hochladen
		
	- Idee für RL-Training: Pauschalen Negativ-Reward bei Terminierung ernfernen, aber Terminierung beibehalten

-----------------------------------------
19.10.		
	- DDPG-Agenten nach 18000 Episoden abgebrochen: max. 1xx reward
	- gleichen DDPG-Agenten ohne -50 Terminierungsreward gestartet, Terminierung an sich unverändert

-----------------------------------------
20.10. - 22.10.	
	- DDPG-Agent ohne -50 terminal reward hat bis zu 180 reward errreicht, ist aber nach mehr als 11000 Ep. auf 0 konvergiert
	- PPO-Agenten mit 200 breiten NNs laufen gelassen, kein gutes Ergebnis
	- Begonnen, results-Abschnitt zu überarbeiten
			

-----------------------------------------
23.10. - 24.10.	
	- nichts, aber auch wirklich gar nichts geschafft


-----------------------------------------
25.10.		
	- Oberseminar:
		- Schriftlich soweit mit Umfang und Detailgrad zufrieden, es fehlt noch ein ausformulierter Results-Abschnitt, v.a. RL-Training 
		- Bei inhaltlicher Arbeit schwierig einen Schlussstrich zu ziehen, da RL-Training wenig Fortschritte macht und Experimente viel Zeit benötigen
			--> Ideen zur Verbesserung sind vielzahlig vorhanden, aber es fehlt die Zeit diese alle Testen zu können
				Training muss nun mal 3-6h laufen, damit erste Ergebnisse sichtbar werden
		
		- Vorschläge von Paula und anderen Teilnehmern:
			- Reward-Signale normieren 
			- Bewegung in x-Richtung stärker belohnen
			- mehrere unterschiedliche Sequenzen über Nacht ausprobieren(Skript anlegen ?)
			
		- wenn möglich, überall doi-Link angeben --> wenn nicht vorhanden aber auch nicht schlimm
		- VORTRAG BEIM PRÜFUNGSAMT ANMELDEN (min. eine Woche vorher) !!!!   --> Anfrage schon gestellt
		- Termin mit Malte und Julius für nächte Woche finden, da Mi. Feiertag
		
	
-----------------------------------------
26.10.		
		- Gleichungen für Bein-Trajektorien korrigiert
		- Introduction-Kapitel endlich fertig überarbeitet (incl. Paulas Anmerkungen)
		- Background bis auf RL section ebenfalls abgeschlossen
		
		- Parameter der Reward-Funktion in rl_setup Skipt mit aufgenommen
	
-----------------------------------------
27.10. - 29.10.
		- nicht viel inhaltlich
		- Feedback von Malte erhalten
		- RL-Agent sieht soweit recht gut aus(Version mit 50x Reward für v_x)
		
	
-----------------------------------------
30.10.	
		- Überarbeitung Background fertig (es fehlt noch Absatz zu RL-Agorithmen und Gait-Arten)
		- Mail an Malte mit Frage bezüglich Treffen diese Woche geschickt
		- Antwort vom Prüfungsamt erhalten --> mit Datum des Vortrags geantwortet, warte noch auf Bestätigung der Anmeldung

-----------------------------------------
31.10.		
		- Überarbeitung des Hauptteils: 
			- Hexapod-Section
			- Environment-Section
			- Locomotion Controller - Section bis zur einzelnen Control Unit
		
		- Antwort von Malte erhalten, Treffen heute um 15:30 Uhr
	
			--> Fragen für letztes Treffen: 
					- 1. Anmerkung von Malte bezüglich "Reproduzierbarkeit", Introduction. Wie verstehen ? -->steht in Problemstellung, hört sich da eher negativ an
					- muss ich Arbeit rechzeitig abgeben, sodass ihr diese vom Prüfungsamt erhaltet oder reicht PDF ? --> PDF reicht, aber Tag vorher bei PA abgeben
					- Länge der Arbeit, was ist akzeptabel ? 50 Seiten zu viel ? --> 30-40 Seiten ausgeführten Text
					- Immer im Präsens Aktiv schreiben ? (z.B. auch hierbei: We reorganize and encapsulate logical groups into...)
						--> an Malte 2 Sätze per E-Main schicken(S.19), möchte sich mit Paula absprechen
							--> Antwort: "Präsens passt"
					- Begründungen im Methods-Teil: z.B. bei Kollisionen zwischen Beinen: "We realize this might result in physical inaccuracies, but ..." ?
						--> Auf jeden Fall ok, sollte sich aber nicht zu viel vermischen
					- Vortrag, wie lang ? 30 min. ? (Zwischenvortrag war glaube ich ca. 40-45 min.) --> 30 min, Background reduziert, dafür viel Ergebnis und Bewertung

-----------------------------------------
01.11.				
		- Rework vom Locomotion Controller beendet
		- kleinen DDPG-Agenten gestertet (N=32)
		
-----------------------------------------
02.11.				
		- Geplant:
			- Simulink-Modell Abgabe-fertig machen   				x
			- Sanaodos Footfall-Pattern testen						O
			- Überarbeitung von Methods beenden						X
			- Überarbeitung/ Erweiterung von Results beginnen		O
		
		- Simulink-Modell begonnen aufzuräumen/ auf Bezeichnungen aus schriftlichem Teil angepasst
		- Grafiken üerarbeitet, neue Grafiken erstellt
		- RL section weiter ausgebaut, MDP und POMDP
		- Tabellen in eigenem "Model Parameter"-Abschnitt untergebracht
		- immer noch RL-Trainingsläufe, wieder kleinere DDPG-Agenten am ausprobieren (n=64, std.dev.=0.15, ExpBuf: 1e6 --> 1e5)
		
-----------------------------------------
03.11.				
		- Geplant:
			- Überarbeitung Results
			- Ergebnis-Daten erstellen
			- Grafiken erstellen/ überarbeiten						x
			- Notizen alle nochmal durchlesen
			- Results und Fazit überarbeiten/ erweitern
		
		- PRIO 1:
			- Methods abschließen
			- Results beginnen
			- Footfall-Pattern erstellen
			
			
		- Absatz über DDPG und PPO erstellt
		- RL-Abschnitt in den methods fertiggestellt
		
		
			
		
-----------------------------------------
04.11.				
		- Geplant:
			- Overall-Struktur Diagramm aufmalen; Topics und Pfeile
			- Results, Fazit, Ausblick
		
		- 
		
-----------------------------------------
05.11. 	
	
		- Geplant:
			- alle Quellen checken
			- DOI-Links zu Quellen hinzufügen
			- alle TODOs lösen
			- CD-Daten zusammenstellen
		
		- 
		
-----------------------------------------
06.11.	LETZTER TAG DER BEARBEITUNG + DRUCKTAG
			
		- Geplant:
			- letzte Änderungen
			- Korrektur lesen
			- alle Links testen
			- Rechtschreibfehler-Scan (manuell oder auto)
			- Bachelorarbeit 4-Fach drucken lassen
			- 4 CDs mit gesamtem Inhalt brennen
			- Abschluss-Vortrag beginnen mit Struktur
			- Vortragsstruktur an Malte schicken
			- PDF der Bachelorarbeit an Malte und Paula schicken
			
		- 
		
-----------------------------------------
07.11.	ABGABE-TAG

		- Geplant:
			- Abgabe der Arbeit im Prüfungsamt
			- Abschlussvortrag weiter bearbeiten

		- 
		
-----------------------------------------
08.11.	ABSCHLUSSVORTRAG			
	
		- Geplant:
			- Abschluss-Vortrag vollenden und üben
			- Vortrag um 16:30 Uhr im Oberseminar halten
				
				
				#
				###
				#####
	###################	
	####################    Ende der Bachelorarbeit
	###################   	
				#####
				###
				#
---------------------------------------------------------------------------------------------------------------------------
------------ZEITPLAN-------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------


-----------------------------------------1
07.08 - 13.08.
	- In RL und Simulinks RL-Agent einlesen															X
	- Background weiterschreiben																	x
	- min. 2 Paper lesen																			x
	- Hexapod PID-Controller tunen																	X
	- Sanandos Modell ansehen, ausprobieren ob einfach mit meinem verbindbar						x

-----------------------------------------2
14.08 - 20.08.
	- Experimente mit RL-Agent beginnen																X
	- Background weiterschreiben																	X
	- Modell überarbeiten(mehrfach verwendete Subsysteme als Library anlegen ?, etc....)			X
	- Paper lesen																					x
	- Zwischenvortrag beginnen (?) --> Ja !															X

-----------------------------------------3
21.08 - 27.08.
	- Hoffentlich bis hier erste Erfolge mit RL-Learning											x
	- Background sollte großteils fertig sein														X
	- Related Works weiterschreiben																	x
	- Im Methods-Teil schonmal anfangen, grundsätzlichen Aufbau und Gait-Entwicklung zu erklären	X
	- Zwischenvortrag bearbeiten, großteilig fertigstellen											X
-----------------------------------------4
28.08 - 03.09.
	- RL-Learning fortführen, Fortschritt festhalten !												x
	- Related Works großteils fertig																x
	- Methods-Teil weiterschreiben																	X
	- Results kann teils angefangen werden(das Modell an sich kann bereits bewertet werden)			x
	- Version des Vortrags an Malte und Paula schicken												X
	- Zwischenvortrag halten (31.08)																X

-----------------------------------------5
04.09 - 10.09.
	- Ziel bis hierhin fertig mit der inhaltlichen Arbeit zu sein
	
	Update(31.08.):
		- Expose Korrekturen von Paula umsetzen
		- Hexapod-Inputs anpassen																	X
		- RL im Background fertigstellen															x
		- Sanandos Modell genau anschauen(nach seinem Vortrag)
		- Arbeit offiziell anmelden																	X

-----------------------------------------6
11.09 - 17.09.
	- nur Schreiben
	- Abschlussvortrag vorbereiten (?)

-----------------------------------------7
18.09 - 24.09.
	- nur Schreiben



(Hoffentlich nicht mehr benötigt)
-----------------------------------------
25.09 - 01.10.





---------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------

TODOs vor Abgabe:
	- alle TODOs lösen und entfernen
	- Die kompletten Notizen nochmal durchlesen und nach ungelösten Problemen Ausschau halten
	- Bachelorarbeit komplett(auch Quellen !) durchlesen, keine Musik, keine Ablenkung, rein gar nichts
	- alle Links im Dokument anklicken und überprüfen
	// - (edes einzelne .tex Dokument durchlesen(auf auskommentierte Sektionen achten)
	- PDF der Arbeit in Rechtschreibprüfer eingeben(?)