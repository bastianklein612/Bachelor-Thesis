%!TEX root = ../thesis.tex
\chapter{Results}
\label{ch:results}

In this chapter we present the results we compiled during the course of our work.
This includes an evaluation of the developed hexapod model and its motion controller as well as review of the reinforcement learning results obtained and lessons learned from observing the training process.

\section{Evaluation: Hexapod and controller}
During the development process of the hexapod model, and also while working on the motion controller, we observed that the conceived model architecture enables us to quickly modify or add features to the hexapod.
As we had to retrospectively add additional sensory output for rotational acceleration to each of the robots joints, these can serve as an example for the described flexibility. 
As the hexapod leg is defined in the custom library, it is only necessary to enable the respective sensor reading one each of the legs three joints and add the new signals to the already existing sensory information bus. 
There is then no need to modify any of hexapods actual legs as Simulink automatically updates all library-linked blocks.
In general, the library of custom subsystems has proven to be a useful tool, reducing development time and promoting modularity.
The encapsulation of the hexapod model as a whole into a single, top-level subsystem block which only shows necessary inputs and outputs, provides the developer with a simple to understand interface.
This also gives us the ability to duplicate the hexapod system without any effort, which could prove useful in future research as it enables the system to easily be used in multi-agent simulations.

Concerning the actual simulation of the model, it can be said that in most cases where the robot uses a stable gait pattern for locomotion, a simulation step size of 0.5 ms yields stable results and runs at close to real time.
Given an untrained, exploring RL agent as the models operator, a step size of 0.25 ms is preferable, as sudden twitching and jerking motions might occur, which can lead to simulation instabilities and exceptions.
Similar to the results of \cite{thilderkvist2015motion}, we start to observe these simulation problems at larger step sizes ($> 0.5 ms$), although not of the same significance and frequency as described by them. 
The most common reason we observed for the exceptions is a collision detection failure occurring when two rigid bodies move into each other faster than the solver can observe, due to its limited temporal resolution.
During several ten-thousand simulation episodes run at a step-size of 0.25 ms, we 
Overall, the behaviour of our robot model seems, to the best of our knowledge, but without the co-examination of a real world counterpart, physically accurate.

We attribute the unsatisfactory results \cite{thilderkvist2015motion}  experienced from the spatial friction forces to be most likely linked to deciding on a step size too large for stable simulations.
As they also work with an earlier MATLAB version, using Simscape's predecessor SimMechanics, the problems might have arisen from a possibly more primitive physics model.


\todo{major rewrite and update}
\section{Evaluation: RL training}
Implementing a RL agent which yields satisfying results proved to be more difficult than anticipated.
There exists a large set of parameter which have to be considered when setting up an agent and as training can take a long time, finding the right parameters is a time-consuming task which requires careful planning.
There are several different learning algorithms to choose from, each with their own set of parameters as well as the NN architecture of the agent. 

We used the \textbf{D}eep \textbf{D}eterministic \textbf{P}olicy \textbf{G}radient (DDPG) algorithm, an actor-critic, model-free, online, off-policy RL method.
The actor network consists of a 12-wide observation input layer followed by 3 100 neuron wide fully-connected layer, in between each of them a ReLU-layer to introduce non-linearity.
Concluding the actor network is an output layer consisting of 6 neurons, one for each action channel.
The architecture of critic network is more complicated, as it has to take in more informations.
At the start this network consists of two separate branches, one taking as inputs X and one Y.
On both branches the input layers are followed by a fully-connected and a ReLU layer.
At this point the two branches are connected by a simple addition layer.
After the add-layer, another 2 combinations of fully-connected and ReLU layer follow, concluding with a single output neuron, representing the critics reward estimation.
All of the actor and critic layers are, if not mentioned otherwise, 100 neurons wide.
The NN architectures are similar to the ones used in \parencite{AUTHOR}, but we changed some details to better fit our purpose.

We used a learning rate of $3e^{-4}$ and $1e^{-3}$ for the actor and critic respectively and a discount factor of 0.99.
The agents sample rate is set at 0.05, meaning the agent samples the environment and takes an action every 50 ms.
A more detailed summary of the RL parameters is given in table \ref{table: DDPG parameters}.


\begin{table}
	\centering
	\begin{tabular}{| l | c |}
		\hline
		\textbf{Parameter} & \textbf{Value}\\
		\hline
		\hline
		Actor learn rate & $3e^{-4}$ \\
		Critic learn rate & $1e^{-3}$ \\
		Discount factor &  0.99 \\
		Mini-Batch-Size & 16 \\
		Experience buffer & $1e^6$\\
		\textsigma \ (UB) & 0.2 \\
		\textsigma-decay (UB) & $1e^{-6}$ \\
		
		\hline
	\end{tabular}
	\caption{DDPG parameters. Uhlstein-Beck (UB) noise model is used to introduce noise into the agents exploration}
	\label{table: DDPG parameters}
\end{table}


\begin{figure}[h]
	\begin{subfigure}{.5\textwidth} % this sets the figure to be max half the width of the page
		\centering
		% include first image
		\includegraphics[width=.9\linewidth]{example-image-a}  % this sets the image to fill 90% of the available space -> 45% of the line width in total. 
		\caption{}
		\label{figure: RL a}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
		% include second image
		\includegraphics[width=.9\linewidth]{example-image-b}  
		\caption{}
		\label{figure: RL b}
	\end{subfigure} 
	\begin{subfigure}{\textwidth}
		\centering
		% include third image
		\includegraphics[width=.45\linewidth]{example-image-c}   % this width should be half of the width of the other two images
		\caption{}
		\label{figure: RL c}
	\end{subfigure}
	\caption[]{DDPG learning graphs. (a) Example 1. (b) Example 2. (c) Example c.}
	\label{figure: DDPG learning graphs}
\end{figure}



When we first started working on the RL process, te result were not very promising. The agent did stabilize, meaning it did not jump around randomly, but it was not able to find an efficient gain.
The agent never learned to reliably move a leg forward if it was starting to drag behind due to reaching the PEP.

Early on in the RL setup process we realized that the current inputs into the hexapod model are not suitable for reinforcement learning.
These initial inputs, frequency, duty cycle and offsets were better suited to being statically optimized instead of learned.
The new inputs, one signal per leg to initiate the swing phase, are much more suited for RL, as they enable a more complex and dynamic interaction with the robot.
Only after implementing these changes were we able to record the first successes in learning.


\textit{MATLABs Parallel Computing Toolbox} provided a significant boost in learning performance.
By being able to run 8-16 agents in parallel , one per processor core or multiple in separate threads, we were able to speed up the learning process significantly.
This allowed us to test several different agent configurations in an acceptable amount of time and also enabled us to refine well performing agents by running more episodes.

The simulation studies and RL training we run in this work are accomplished with a desktop computer powered by an Intel i7-11700K CPU, 32 GB of RAM and a NVIDIA RTX 2060 GPU.

Concerning MATLAB, we used the version 2023a for all of our work.


The final, successful agent trained for x episodes and a total of x simulation steps. 
This means the agent trained for about x hours of real time to achieve the results.

We are confident that, given more time to carefully tune parameters and reward, the training time could be reduced significantly and performance improved.

