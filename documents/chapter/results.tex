%!TEX root = ../thesis.tex
\chapter{Results}
\label{ch:results}

In this chapter we present the results we compiled during the course of our work.
This includes an evaluation of the developed hexapod model and its locomotion controller, as well as review of the RL training results obtained and lessons learned from observing the training process.

\section{Evaluation: Hexapod model}
During the development process of the hexapod model and locomotion controller, we observed that the conceived modular architecture enabled us to quickly modify and/or add components.
An example of this flexibility is provided by the retrospective addition of sensor readings for the rotational acceleration of the \textalpha-joints.
As the hexapod leg is defined as a library component, it was only necessary to enable the respective sensor reading one the joint block and add the new signal to the already existing sensory information bus. 
There was no need to modify the hexapods legs individually, as Simulink automatically updates all library-linked blocks.
In general, the library of custom subsystems has proven to be a most useful tool, reducing development time and promoting modularity.
The encapsulation of the hexapod model as a whole into a single, top-level subsystem block, that only reveals necessary in- and outputs, provides the developer with a simple to understand interface.
It also gives us the ability to duplicate the complete hexapod system without any effort, which could prove useful in future research as it enables the system to easily be utilized in multi-agent simulations.

Concerning the actual simulation of the model, it can be said that in most cases in which the robot uses a statically defined gait pattern for locomotion, a simulation step size of 0.5 ms yields stable results and runs at close to real time.
Given an untrained, exploring RL agent as the models operator, a step size of 0.25 ms is preferable, as sudden twitching and jerking motions might occur, which can lead to simulation instabilities and exceptions.
Similar to the results of \cite{thilderkvist2015motion}, we begin to observe these problems at larger step sizes ($> 0.5 ms$), although not of the same significance and frequency as described by them. 
The most common reason for exceptions we observed, were collision detection failures.
These occur, when two rigid bodies move into each other faster than the simulation's solver can observe, due to its limited temporal resolution.
During the several ten-thousand simulation episodes performed at a step-size of 0.25 ms during RL training, we did not once observe an exception.
Overall, the behaviour of our robot model seems, to the best of our knowledge, but without the co-examination of a real world counterpart, physically accurate.


We attribute the unsatisfactory results \cite{thilderkvist2015motion} experienced from the spatial friction forces to be most likely linked to deciding on a step size too large for stable simulations.
As they also work with an earlier MATLAB version, using Simscape's predecessor \textit{SimMechanics}, the problems might also have arisen from a possibly more primitive physics model.

To summarize, we were able to create a well-functioning, virtual model of a hexapod robo in Simulink.
Simulations of the model are robust, physically accurate and performant.
Utilizing a modular architecture, the model can easily be adapted to changing requirements.
 

\section{Evaluation: Locomotion Controller}
The locomotion controller we presented in \ref{subsec: Locomotion controller} is the result of several design iterations.
We extensively tested each of the designs to uncover their weakpoints, upon which the next iteration was improved on.

To begin with, the initial design (\textit{MK1}) enabled the robot to walk in a straight line tripod gait.
We observed that the hexapod model, controlled by MK1, would jerk up and down significantly during a movement cycle, as can be seen in [\hyperref[vid: MK1]{MK1}] and \ref{figure: Thorax Height graph, bad tuning}.
During a legs swing phase, the robot's thorax would slowly sink towards the side of the lifted leg and be rapidly pushed upwards again at the end of the swing.
As we defined the leg trajectory relative the thorax and did not consider sensing ground contact or body tilt, the leg on the sinking side would hit the ground earlier than expected by the controller, as the anticipated AEP "sunk" into the ground plane.
We were able to later on attribute this behaviour to the slow reactions of the PID controllers.
These were, given our initial tuning parameters ($K_p = 3, K_i = 0.2, K_d = 1, N = 100$), unable to follow their assigned trajectories. 
In addition, the MK1 controller version requires a lot of calculation time per simulation step, resulting in slow simulations.
The observed cause of this being the computationally intensive, iterative IK block provided by Simulink, as we have already elaborated on in \ref{subsubsec: IK Solver}.

With the subsequent design iteration \textit{MK2}, we focused on introducing tetrapod and wave gait as well as increasing simulation speed.
Implementing the gait patterns proved to be straightforward, as, resulting from the modular architecture, solely the leg coordination of the static gait definition had to be modified.
The resulting simulations of the robot walking in tetrapod and wave gait can be observed in [\hyperref[vid: MK1]{MK2}].

The decision to implement our own, custom analytical IK solver yielded a significant performance boost.
Comparing the MK1 and MK2 controllers, we are able to achieve an up to 5x speed up in simulation time.
Running a 30 second simulation of a hexapod walking in tripod gait, controlled by MK1, required a full 143 seconds of real compute time, while the same simulation takes about 38 seconds to run when the hexapod is controlled by MK2.

With the final non-RL controller design, termed \textit{MK3}, we improved the stability of the hexapods locomotion as well as introduced the capability for omnidirectional movement.
By simply re-tuning the PID controller parameters to the values displayed in \ref{table: PID parameters}, we were able to reduce the jerking behaviour of the robot far enough to almost not being visually noticeable.
A comparison of the thorax's height differences during movement, when under control of the MK2 and MK3 controllers, can be seen in Fig. \ref{figure: Thorax height graphs}.


\begin{figure}[h]
	\begin{subfigure}{\textwidth} % this sets the figure to be max half the width of the page
		\centering
		% include first image
		\includegraphics[width=.9\linewidth]{ThoraxHeight_firstPID.PNG}  % this sets the image to fill 90% of the available space -> 45% of the line width in total. 
		\caption{}
		\label{figure: Thorax Height graph, bad tuning}
	\end{subfigure}
	
	\begin{subfigure}{\textwidth}
		\centering
		% include third image
		\includegraphics[width=.9\linewidth]{ThoraxHeight_retunedPID.PNG}   % this width should be half of the width of the other two images
		\caption{}
		\label{figure: Thorax height graph, re-tuning}
	\end{subfigure}
	\caption[Thorax height graphs]{(a) Thorax height graphs of hexapod robot controlled by (a) MK2 and (b) MK3 controller.}
	\label{figure: Thorax height graphs}
\end{figure}

Our final, MK3 locomotion controller is able to control the hexapod model in tripod, tetrapod as well as wave gait.
The swing-stance cycle frequency ($f_c$) and the duty cycle percentage ($P_\text{swing}$) can be changed, even dynamically during simulation.
We demonstrate this dynamic frequency alteration with a hexapod model walking in wave gait with varying cycle frequency.
The controller initiates movement with at $f_c = 0.1\ \text{Hz}$, slowly increasing over 20 seconds linearly to $f_c = 0.5\ \text{Hz}$. 
It is further also possible to redefine the step height and AEP/ PEP positions, opening up possibilities for more dynamic control algorithms.
As already mentioned, the final controller is capable of instructing omnidirectional movement, as we demonstrate in a study of the robot walking in a circle.
Additional studies of the hexapod robot performing all predefined gait patterns at different cycle frequencies were performed as well.
We captured all of the simulations mentioned above as videos which can be found at [\hyperref[vid: MK1]{MK3}].

Concerning the hexapods ability to navigate obstacles or uneven terrain, it has to be noted that the controller does not possess any means of observing environmental details.
It is not able to recognize obstacles in it's path and therefore struggles to overcome even small objects.
We explored the robots performance at traversing a 10 cm wide an 4 cm high beam, perpendicular to the robots movement direction.
Summarizing the findings of simulations in all available gaits, the hexapod walking in tripod gait was able to eventually overcome the obstacle, while it struggled to do so in both tetrapod and wave gait, given the allotted timespan of 60 s.
We attribute the tripod gait's success to the fact that it presents the lowest number of legs in stance at any point in time, as we observed that legs in stance often get caught on the obstacle.
Due to the nature of our controller, legs in their stance phase are rather reluctant to deviation from their trajectories.
Thus, when caught in from of the obstacle, they prevent the robot from moving forward.
Videos of the obstacle traversal simulations can be observed in [\hyperref[vid: MK1]{MK3}].


\section{Evaluation: RL training}

Implementing a RL agent yielding satisfying results proved to be more difficult than anticipated.
As there exists a large set of parameter to be considered during setup, finding a well-performing set has shown to be a time-consuming task.
This was reinforced by the fact that, to observe the performance of a set of parameters, we found an agent required at least 1-2 k episodes of training, equating to several hours of runtime given the hardware used.
Additionally, as we ran experiments with two different RL algorithms, we were required to tune two separate sets of parameters.
In total, we trained over 30 different agent configurations.
As we are not able to present all the results obtained, a table providing information on all documented configurations is included in the appendix (\emph{\nolinkurl{RL_agent_configs.xlsx}}).\\
In the following sections, we will present the findings obtained from the performed RL training experiments, focusing on a selected subset of training runs.

\subsection{Reward Exploitation}
A general problem, which is often observed in RL, is an agent exploiting a poorly-defined reward function \parencite{silver2015}.
This commonly leads to agents exhibiting undesirable behaviour, even though according to the reward function they are performing well.
We experienced this exploitation behaviour during development as well.
As our reward function initially did neither take into account the robots energy consumption ($W$) nor height differences of the thorax during movement ($ \Delta_h$), early RL agents exploited this simpler reward function by learning to jump.
More specifically, we observed the robot to exhibit a behaviour consisting of small, abrupt hops, using it's hind legs to propel itself forward.
We provide a recording of the observed behaviour in [\hyperref[vid: MK4]{MK4}], the training graph of the agent can be seen in \ref{}.
In the graph, the occurrence of the exploit is clearly visible as a spike in the average reward that quickly breaks down again.

In the further development process, we refined the reward function weights and included $W$ and $\Delta_h$ to encourage the RL agents to be more resourceful. 
Our final reward function weights, with which we obtained the most promising results, are displayed in table \ref{table: Reward weights}.


\subsection{Learning Behaviour}
The phenomenon of sharp spikes in the average reward as mentioned above, persisted throughout our training runs, repeating on several different agent configurations.
A training graph depicting this behaviour can be seen in Fig. \ref{}.
We suspect these reward plunges to be related to the agent exploring too rapidly/ changing it's policy to fast, resulting in a loss of the already achieved progress \parencite{silver2015}.
However, were cannot provide a definite prove of this cause, as we are not able to reliably prevent the behaviour from occurring.
On the other hand, in stark contrast to the phenomenon just explained, we also observed "flatlining" of the average reward.
An agent would often learn well initially, but after a certain number of episodes converge to an average reward well below that of individual episodes.
Interestingly, this behaviour could very well be attributed to the agent's policy updates not being large enough, such that it is unable to leave a locally optimal policy only based on a few episodes of high reward \parencite{silver2015}.
Despite our efforts to find a middle ground between these two extremes by changing the agent's learning rate an introducing varying amounts of randomness into it's decision process, we were not able to find a well-working configuration.


\subsection{Best Result}
Given the reward function weights in table \ref{table: Reward weights}, a sample rate of 0.05 s and an episode length of 256 steps, the best result we were able to achieve with an RL agent was a maximum episodic reward of 310 units.
In this training run, performed with a DDPG agent with parameters listed in \ref{table: DDPG parameters}, the average reward did neither spike nor converge to a suboptimal policy, as can be seen in \ref{}.
Even though, the average reward does not seen to reliably increase.
Observing the agent's performance in simulation, it's behaviour does not seem to constitute a stable gait pattern.
The hexapod uses various legs to slowly push itself forward, seemingly uncoordinated.
The agent appears to be unable to notice a leg finishing it's stance phase, as often several legs drag behind, due to them reaching the PEP and not being lifted back up again.
A demonstration of the best performing policy obtained from this training run, can be seen in [\hyperref[vid: MK4]{MK4}].
Comparing this result with a hexapod walking in a statically defined tripod gait, which obtains a reward of $\sim785$ units given the same sample rate, episode length and steps, the RL policy performs about 40\% as well as the static gait.


{\def\arraystretch{1.4}\tabcolsep=5pt
	
	\begin{table}[h!]
		\parbox{.4\linewidth}{
			\centering
			\begin{tabular}{| c | c |} 
				\hline
				\textbf{Training params.s} & \textbf{Value} \\ [0.5ex] 
				\hline
				\hline
				Sample time & 0.05 s \\ 
				
				Episode steps & 512 \\
				
				\hline
				\textbf{Agent params.} &  \\ [0.5ex] 
				\hline
				
				Learn Rate & $\num{3e-4}$ \\
				
				Episode steps & 512 \\
				
				Exp. Buf. length & $\num{1e6}$ \\
				
				Discount factor &  0.99 \\
				
				Mini-Batch size & 32 \\
				
				NN-width & 128 \\
				
				\hline
				\textbf{Noise params.} &  \\ [0.5ex] 
				\hline
				
				\textsigma\footnote{\textsigma: Standard Deviation} & 0.05 \\
				
				\textsigma-decay\footnote{\textsigma-decay: Decay rate of \textsigma. Each sample time step k, \textsigma decays according to following formula: $\sigma_{k+1} = \sigma_k \cdot (1 - \textbf{\textsigma-decay})$} & 1e-6 \\
				
				\hline
			\end{tabular}	
			\caption[DDPG agent parameters]{}
			\label{table:DDPG parameters}
		}
	\end{table}
}

{\def\arraystretch{1.4}\tabcolsep=5pt
	\begin{table}[h!]
		\centering
		\begin{tabular}{| c | c |}
			\hline
			\textbf{Weight} & \textbf{Value}\\
			\hline
			\hline
			$c_{s_x}$   		&  1	\\
			$c_{v_x}$   	    &  50	\\
			$c_{v_y}$  		    &  1	\\
			$\Delta_h$  		&  1	\\
			$c_W$ 				&  0.025\\
			\hline
		\end{tabular}
		\caption[Reward weights]{}
		\label{table: Reward weights}
	\end{table}
}



\subsection{Further Remarks}
As we changed the reward function several times in attempts to improve it's ability to accurately describe the performance of an agent, the individual training results can only be compared to a limited extend.
Concerning the two policy optimization algorithms used, DDPG and PPO, there were no significant differences in their performance to be observed.
We therefore abstain from comparing the algorithms separately.

%We used the \textbf{D}eep \textbf{D}eterministic \textbf{P}olicy \textbf{G}radient (DDPG) algorithm, an actor-critic, model-free, online, off-policy RL method.
%The actor network consists of a 12-wide observation input layer followed by 3 100 neuron wide fully-connected layer, in between each of them a ReLU-layer to introduce non-linearity.
%Concluding the actor network is an output layer consisting of 6 neurons, one for each action channel.
%The architecture of critic network is more complicated, as it has to take in more informations.
%At the start this network consists of two separate branches, one taking as inputs X and one Y.
%On both branches the input layers are followed by a fully-connected and a ReLU layer.
%At this point the two branches are connected by a simple addition layer.
%After the add-layer, another 2 combinations of fully-connected and ReLU layer follow, concluding with a single output neuron, representing the critics reward estimation.
%All of the actor and critic layers are, if not mentioned otherwise, 100 neurons wide.
%The architectures of the actor and critic networks have been adopted from example applications created by \cite{matlabDDPGExample, matlabPPOExample}.

%We used a learning rate of $3e^{-4}$ and $1e^{-3}$ for the actor and critic respectively and a discount factor of 0.99.
%The agents sample rate is set at 0.05, meaning the agent samples the environment and takes an action every 50 ms.
%A more detailed summary of the RL parameters is given in table \ref{table: DDPG parameters}.

{\def\arraystretch{1.4}\tabcolsep=5pt
\begin{table}
	\centering
	\begin{tabular}{| l | c |}
		\hline
		\textbf{Parameter} & \textbf{Value}\\
		\hline
		\hline
		Actor learn rate & $3e^{-4}$ \\
		Critic learn rate & $1e^{-3}$ \\
		Discount factor &  0.99 \\
		Mini-Batch-Size & 16 \\
		Experience buffer & $1e^6$\\
		\textsigma \ (UB) & 0.2 \\
		\textsigma-decay (UB) & $1e^{-6}$ \\
		
		\hline
	\end{tabular}
	\caption[DDPG parameters]{DDPG parameters. Uhlstein-Beck (UB) noise model is used to introduce noise into the agents exploration}
	\label{table: DDPG parameters}
\end{table}
}


\begin{figure}[h]
	\begin{subfigure}{\textwidth} % this sets the figure to be max half the width of the page
		\centering
		% include first image
		\includegraphics[width=\linewidth]{graphs/cropped/exploitGraph_cropped.PNG}  % this sets the image to fill 90% of the available space -> 45% of the line width in total. 
		\caption{}
		\label{figure: RL a}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		% include second image
		\includegraphics[width=\linewidth]{graphs/cropped/firstSuccess_146reward_cropped.PNG}  
		\caption{}
		\label{figure: RL b}
	\end{subfigure} 
		\caption[Training graphs (1)]{Training graphs (1): Each graph depicts the learning curve of a RL agent. The dark blue graph represents the average reward over a running average of 250 episodes, while each data point on the light blue graph represents the reward obtained during a specific episode. (a) Jumping exploit learning curve. (b) Reward spiking.
		}
	\label{figure: DDPG learning graphs 1}
\end{figure}

\begin{figure}[h]
	\begin{subfigure}{\textwidth} % this sets the figure to be max half the width of the page
		\centering
		% include first image
		\includegraphics[width=\linewidth]{graphs/cropped/flatlining_cropped.PNG}  % this sets the image to fill 90% of the available space -> 45% of the line width in total. 
		\caption{}
		\label{figure: RL c}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
		\centering
		% include second image
		\includegraphics[width=\linewidth]{graphs/cropped/lastAgent_graph_cropped.PNG}  
		\caption{}
		\label{figure: RL d}
	\end{subfigure} 
	\caption[DDPG training graphs (2)]{DDPG training graphs (2):  (a) Reward flatlining (b) lastAgent.}
	\label{figure: DDPG learning graphs 2}
\end{figure}


\subsection{Hardware}
The simulation studies and RL training runs were all performed on a desktop computer powered by an Intel i7-11700K CPU, 32 GB of RAM and a NVIDIA RTX 2060 GPU.
Concerning MATLAB, we used version 2023a for all of our work.

