%!TEX root = ../thesis.tex
\chapter{Conclusion and Outlook}
\label{ch:conclusion}

In the last chapter of this thesis, (we summarize our results), evaluate how well the developed model and controller work as a foundation for further research and discuss the performance of the reinforcement learning agent in coordinating leg movements.
Finally, we will provide an outlook towards possibly interesting future expansions to our work.

\todo{Results chapter already exists}
%\todo{Maybe do not use sections(these are equal to thesis of Josef Treus[Power Attacks])}

\section{Conclusions}
In this section we draw the conclusions out of our obtained results.
We also divide these into findings about the model + locomotion controller and the RL training.

\subsection{Model and Controller}
MATLAB Simulink proved to be a versatile and reliable platform for the modelling and simulation of robotic systems.
The development of the hexapod model was greatly sped up by the underlying MATLAB framework and the utilisation of the provided toolboxes.
The ability to write MATLAB-scripts to automate several task, like the initialisation of the hexapod model or the setup of the RL agents and surrounding infrastructure. 

The developed model proved very robust in the face of constant twitching caused by exploring RL agents.
\todo{Evaluate MATLAB Simulink in more detail}

\subsection{RL Leg Coordination}
\textit{MATLABs Parallel Computing Toolbox} provided a significant boost in learning performance.
By being able to run 8-16 agents in parallel , one per processor core or multiple in separate threads, we were able to speed up the learning process significantly.
This allowed us to test several different agent configurations in an acceptable amount of time and also enabled us to refine well performing agents by running more episodes.

We are confident that, given more time to carefully tune parameters and reward, the training time could be reduced significantly and performance improved.

Defining a reward function which accurately describes an agents performance is a complicated task requiring careful tuning.
The same applies to the configuration of a RL algorithm.

We found the learning of the RL agents to progress quite slow. 
This could probably be improved on significantly by more carefully tuning the agents parameters and a more detailed reward function.

Our suggestions for future researchers attempting to apply RL to the leg coordination problems are the following:
\begin{enumerate}
	\item Do not change the underlying hexapod model during experimentation with different RL agents. 
	These changes make it difficult to compare the performance of RL parameter sets.
	The hexapod model should be meticulously tested prior to the application of RL.
	As our main goal was to provide the best-performing hexapod model, we decided to rather fix mistakes we discovered, sacrificing on comparability .
	
	\item Automate the process of running several agents with different parameter sets. 
	By automating this task, the speed of exploration in the parameter space could be greatly increased.
	
	\item Meticulously collect the data of each agent's performance to be able discover trends on which parameter values might work well.
	
\end{enumerate}

Mentions problems in RL.
%Sparse reward definition might have played a role in the bad performance of the agents \parencite{matheron2019problem}.
\todo{Look at proposed goals at the beginning of thesis, evaluate these}
Since the main goal of this thesis was to develop a functional and robust hexapod model, on which further research can be based, we consider our work as a success.
Concluding, it can be said that the developed hexapod model and locomotion controller proved to be a success, while the approach to the RL problem needs to be reconsidered/ reevaluated.


\section{Outlook - Future Work}

Since the goal of successfully applying RL to the problem of leg coordination was not reached in completion, it might be appealing, as a future work, to focus solely on this problem.
As we only implemented a rudimentary locomotion controller capable of navigation on flat terrain, expanding the controller to more complex, uneven terrain would be a valuable research target.


\begin{itemize}
	
	\item Controller improvements:
		\subitem Curve walking
		\subitem Adaptation to uneven terrain
		\subitem 
	
	\item Expand Reinforcement Learning from just leg coordination towards controlling the whole movement process
		\subitem Improve reward definition/ terminate episodes in which no progress seems to be made after some time(such as in \cite{lillicrap2015continuous})
		\subitem Learn walking from the ground up, meaning the agent controls each of the 18 joints (maybe take steps in between)
		\subitem
	
	\item Use other MATLAB Tools such as Hardware Co-Simulation to test controller on real robot
	
	\item 
	
\end{itemize}