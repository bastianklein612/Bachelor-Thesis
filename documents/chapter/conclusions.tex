%!TEX root = ../thesis.tex
\chapter{Conclusion and Outlook}
\label{ch:conclusion}

In the last chapter of this thesis, (we summarize our results), evaluate how well the developed model and controller work as a foundation for further research and discuss the performance of the reinforcement learning agent in coordinating leg movements.
Finally, we will provide an outlook towards possibly interesting future expansions to our work.

\todo{Results chapter already exists}
%\todo{Maybe do not use sections(these are equal to thesis of Josef Treus[Power Attacks])}

\section{Conclusions}
In this section we draw the conclusions out of our obtained results.
We also divide these into findings about the model + locomotion controller and the RL training.

\subsection{Model and Controller}
MATLAB Simulink proved to be a versatile and reliable platform for the modelling and simulation of robotic systems.
The development of the hexapod model was greatly sped up by the underlying MATLAB framework and the utilisation of the provided toolboxes.
The ability to write MATLAB-scripts to automate several task, like the initialisation of the hexapod model or the setup of the RL agents and surrounding infrastructure. 

\todo{Evaluate MATLAB Simulink in more detail}

\subsection{RL Leg Coordination}
We found the learning of the RL agents to progress quite slow. 
This could probably be improved on significantly by more carefully tuning the agents parameters and a more detailed reward function.

Our suggestions for future researchers attempting to apply RL to the leg coordination problems are the following:
\begin{enumerate}
	\item Do not change the underlying hexapod model during experimentation with different RL agents. 
	These changes make it difficult to compare the performance of RL parameter sets.
	The hexapod model should be meticulously tested prior to the application of RL.
	As our main goal was to provide the best-performing hexapod model, we decided to rather fix mistakes we discovered, sacrificing on comparability .
	
	\item Automate the process of running several agents with different parameter sets. 
	By automating this task, the speed of exploration in the parameter space could be greatly increased.
	
	\item Meticulously collect the data of each agent's performance to be able discover trends on which parameter values might work well.
	
\end{enumerate}

Mentions problems in RL.
%Sparse reward definition might have played a role in the bad performance of the agents \parencite{matheron2019problem}.



\section{Outlook - Future Work}



\begin{itemize}
	
	\item Controller improvements:
		\subitem Curve walking
		\subitem Adaptation to uneven terrain
		\subitem 
	
	\item Expand Reinforcement Learning from just leg coordination towards controlling the whole movement process
		\subitem Improve reward definition/ terminate episodes in which no progress seems to be made after some time(such as in \cite{lillicrap2015continuous})
		\subitem Learn walking from the ground up, meaning the agent controls each of the 18 joints (maybe take steps in between)
		\subitem
	
	\item Use other MATLAB Tools such as Hardware Co-Simulation to test controller on real robot
	
	\item 
	
\end{itemize}