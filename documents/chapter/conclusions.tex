%!TEX root = ../thesis.tex
\chapter{Conclusion and Outlook}
\label{ch:conclusion}

In the last chapter of this thesis, (we summarize our results), evaluate how well the developed model and controller work as a foundation for further research and discuss the performance of the reinforcement learning agent in coordinating leg movements.
Finally, we will provide an outlook towards possibly interesting future expansions to our work.

\todo{Results chapter already exists}
%\todo{Maybe do not use sections(these are equal to thesis of Josef Treus[Power Attacks])}

\section{Evaluation}
Mentions problems in RL.
Sparse reward definition might have played a role in the bad performance of the agents \parencite{matheron2019problem}.




\subsection{Model and Controller}
MATLAB Simulink proved to be a valuable and reliable platform for the modeling and simulation of hexapod robots.
\todo{Evaluate MATLAB Simulink in more detail}

\subsection{RL Leg Coordination}
We found the learning of the RL agents to progress quite slow. 
This could probably be improved on significantly by more carefully tuning the agents parameters and a more detailed reward function.



\section{Outlook - Future Work}



\begin{itemize}
	
	\item Controller improvements:
		\subitem Curve walking
		\subitem Adaptation to uneven terrain
		\subitem 
	
	\item Expand Reinforcement Learning from just leg coordination towards controlling the whole movement process
		\subitem Improve reward definition/ terminate episodes in which no progress seems to be made after some time(such as in \cite{lillicrap2015continuous})
		\subitem Learn walking from the ground up, meaning the agent controls each of the 18 joints (maybe take steps in between)
		\subitem
	
	\item Use other MATLAB Tools such as Hardware Co-Simulation to test controller on real robot
	
	\item 
	
\end{itemize}