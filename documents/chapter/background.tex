%!TEX root = ../thesis.tex
\chapter{Background}
\label{ch:background}

\todo{MALTE: Small introduction to chapter; what to expect}

\section{Hexapods} \label{sec: Hexapods}

\begin{figure}[!h]
	\begin{subfigure}{.5\textwidth} % this sets the figure to be max half the width of the page
		\centering
		% include first image
		\includegraphics[width=\linewidth]{phantomX_III_overview}  % this sets the image to fill 90% of the available space -> 45% of the line width in total. 
		\caption{}
		\label{figure: PhantomX MKIII}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
		% include second image
		\includegraphics[width=0.85\linewidth]{HexapodLeg.png}  
		\caption{}
		\label{figure: Hexapod Leg Sketch}
	\end{subfigure}
	\caption[Hexapod and hexapod leg]{(a) Trossen Robotics PhantomX MKIII hexapod, predecessor of the MKIV version used in this thesis [\cite{PhantomX_MKIII}] (b) Drawing of a 3-segment, 3-joint hexapod leg.}
	\label{figure: Hexapod and hexapod leg}
\end{figure}

Hexapods are a class of robots featuring 6 legs, inspired by naturally occurring six-legged creatures like insects and arachnids.
Through millions of years of evolution these organisms developed efficient locomotion strategies \parencite{neville2006bipedal}, making them a rich source of inspiration for robotics.
By emulating the biomechanics and behaviour of insects, researchers and engineers are able to create versatile and robust robotic systems capable of navigating challenging terrain \parencite{irawan2011optimal, ouyang2021adaptive, schilling2013walknet}.

Most commonly, each leg of a hexapod consists of 3 segments named coxa, femur and tibia, equivalent to their biological counterparts.
The individual segments are connected to each other and the thorax (main body) by three 1-DoF\footnote{DoF: \textbf{D}egrees \textbf{o}f \textbf{F}reedom} joints, each actuated by an electric servo motor.\todo{Use footnotes for abreviations}
The first joint, hereafter named \textalpha-joint, connects the coxa to the thorax and moves in parallel to the ground, being responsible for the longitudinal placement of the leg.
Coxa and femur are connected by the \textbeta-joint, while the femur and tibia are connected by the \textgamma-joint. 
These joints move orthogonal to the movement plane of the \textalpha-joint. Together, they are responsible for the lateral positioning of the leg.
An example of a hexapod robot and leg based on the described architecture can be seen in Fig. \ref{figure: Hexapod and hexapod leg}.
The nomenclature we introduced in this section and use throughout the thesis to label joints and leg segments was adopted from the works of 
\cite{schilling2013walknet} and \cite{HeterarchicalArchitectureSchilling}.


\section{MATLAB}
\textit{MATLAB\textsuperscript{\textregistered}} is a numerical computing environment and programming language, developed by the company \textit{MathWorks\textsuperscript{\textregistered}}.
It is widely used by researchers and engineers alike in applications such as data analysis and visualisation, algorithm development or the creation of virtual models.
The MATLAB environment provides a multitude of apps and toolboxes to support different domains, for example model-based design, machine learning, signal processing or hardware co-simulation \parencite{MATLAB}.

\subsection{Simulink}
The overview provided in this section is based of the official Simulink documentation \parencite{matlabSimulinkDocumention}.\\
\textit{Simulink\textsuperscript{\textregistered}} is a \textit{MATLAB\textsuperscript{\textregistered}}-based graphical block-diagramming tool. 
It is widely used and plays a crucial role in various engineering and research disciplines.
It provides the user with a versatile platform to design, simulate and analyse complex dynamic systems.
Simulink offers an expansive library of predefined blocks that represent different components and behaviours.
The user connects these blocks via so called signal lines, which transport data in between blocks.
The blocks themselves transform the data provided by the inputs according to their function and output the transformed data to other blocks connected downstream.
Their behaviour can be discrete, like a switch or logic gate which activates when specific signals are pulled high, or represent more complex and continuous functions such as integrators, derivatives or sine waves.
An arrangement of blocks can be encapsulated into a subsystem, thus creating different levels of abstraction.
To enable simple reuse of components, subsystems can be placed in custom libraries.
If such a library object is modified, each linked copy of this subsystem receives the update as well, preventing the developer from having to edit each copy individually.
At any step in the development process, a model can be simulated and analysed.
To receive better insight into the systems internal behaviour, the data of any signal lines can be logged and plotted over time and the simulation speed can be slowed down.

An example of a simple Simulink model is depicted in Fig. \ref{figure: Simulink bouncing ball example}.
The model simulates the behaviour of an elastic ball, which, under the influence of gravity, accelerates downwards and repeatedly bounces of an imaginary plane.
The ball looses energy over time, as represented by the coefficient of restitution.
To visualize the systems dynamics, the balls velocity and speed are plotted on a graph, as indicated by the symbols next to the respective signal lines.

\begin{figure}[h!]
	\begin{subfigure}{.5\textwidth} % this sets the figure to be max half the width of the page
		\centering
		% include first image
		\includesvg[scale=0.6]{Simulink/BouncingBallExample_Simulink.svg}  % this sets the image to fill 90% of the available space -> 45% of the line width in total. 
		\caption{}
		\label{figure: Simulink bouncing ball model}
	\end{subfigure}
	\begin{subfigure}{.5\textwidth}
		\centering
		% include second image
		\includegraphics[width=\linewidth]{Simulink/BouncingBall_PositionAndVelocity.png}  
		\caption{}
		\label{figure: Simulink bouncing ball graphs}
	\end{subfigure}
	\caption[Simulink bouncing ball example]{(a) Simulink model of a bouncing ball created using standard library blocks. (b) Graphs depicting the balls velocity (orange) and position (blue).}
	\label{figure: Simulink bouncing ball example}
\end{figure}


\subsection{Simscape}
The description of the Simscape library in this section follows the official documentation \parencite{matlabSimscapeDocumentation}.
\textit{Simscape\textsuperscript{\texttrademark}} is a Simulink block library developed by \textit{MathWorks\textsuperscript{\textregistered}}., enabling the construction of physical systems within the Simulink environment.
Utilizing Simscape, it is possible to simulate systems such as electric circuits, hydraulics or classical mechanics all within a unified simulation environment.
The library offers a large variety of predefined components like resistors, capacitors, springs, dampers, etc.
To better understand the model we develop in this thesis, Simscape's mechanical components are of the most interest, especially coordinate frames, rigid transforms, joints and rigid bodies.

\begin{figure}[h!]
	\centering
	\centerline{\includesvg[scale=0.8]{Simulink/BouncingBallExample_Simscape.svg}}
	\caption[Simscape bouncing ball example]{Simulink model of the same bouncing ball system as in \ref{figure: Simulink bouncing ball model}, but created using solely Simscape blocks. The simulation of this system is provided in \hyperref[vid: MK1]{MK1}.} 
	\label{figure: Simscape Bouncing Ball Example}
\end{figure}

Coordinate frames are at the base of every Simscape model.
They can be attached to each other using rigid transforms or joints.
Edges similar to the basic Simulink signal lines are used to connect the individual blocks.
These lines do not transport plot-able signal however, but rather represent the structure of the Simscape model.
Joints allow for different DoFs between two frames, depending on the constraints that should be imposed on the system.
Two frames connected by a joint are called base and follower frame.
When a joint is actuated, either by an external or internal force, the follower frame moves relative to the base frame \parencite{thilderkvist2015motion}.
Joints can be actively actuated by providing them with a scalar torque signal or just react to the forces acting upon them.
It is also possible to modify various joint parameters such as internal joint friction, joint limits and spring stiffness, which we will further elaborate on in chapter \ref{ch:methods}.
Rigid transforms, opposed to joints, translate and/or rotate coordinate frames without allowing for any DoF.
They are used to define the initial position and orientation of a frame.
Rigid bodies can be attached to the already existing coordinate frames, providing shape, mass and inertia to the system.
If multiple rigid bodies are present in a simulation, their shapes can also act as collision geometry, allowing for collisions/ interactions between them.

As an example, the same bouncing ball system as shown in Fig. \ref{figure: Simulink bouncing ball model} can be seen in Fig. \ref{figure: Simscape Bouncing Ball Example} constructed using only blocks from the Simscape library. When running this model, a 3D simulation of the bouncing ball is generated which can be slowed down, replayed or saved for later analysis (\hyperref[vid: MK1]{MK1}).
\todo{Add bouncing ball video}

\section{Insect Locomotion}
The locomotion of arthropods, more specifically insects, serves as a remarkable source of inspiration for the field of hexapod robotics.
There exists a vast amount of papers studying the movement patterns of these organisms.
Common research subjects include stick insects (\textit{C. morosus}) \parencite{cruse1990mechanisms}, fruit flies (\textit{D. melanogaster}) \parencite{strauss1990coordination} and cockroaches (\textit{P. americana}) \parencite{delcomyn1971locomotion}. 
The movement of an insect's leg can generally be divided into two phases, called \emph{"Swing"} and \emph{"Stance"} \parencite{schilling2013walknet}.
Together, they form what we will call the \emph{"Swing-Stance Cycle"}, which is depicted in Fig. \ref{figure: Stick insect leg}.


\begin{figure}[h]
	\centerline{\includegraphics[scale=0.4]{morphologyOfStickInsect_sketch}}
	\caption[Morphologic drawing of stick insect]{Morphologic drawing of a stick insect leg (\cite{schilling2013walknet} [Fig.1]).
		 	Angles \textalpha, \textbeta \ and \textgamma \ represent the position of the thorax-coxa, coxa-femur and femur-tibia joint respectively.
			These joints are actuated with three pairs of muscles, called protractor-retractor, levator-depressor and extensor-flexor.
			Also depicted are the Anterior Extreme Position (AEP) and Posterior Extreme Position (PEP).
			Dashed lines indicate the swing and stance movement between PEP and AEP.}
	\label{figure: Stick insect leg}
\end{figure}

While the leg is in the stance phase, it is continuously in contact with the ground and bears a partial load of the insects weight.
During this phase, the leg moves from AEP\footnote{AEP: \textbf{A}nterior \textbf{E}xtreme \textbf{P}osition} to PEP\footnote{\label{footnote: PEP}PEP: \textbf{P}osterior \textbf{E}xtreme \textbf{P}osition}, pushing the body in the desired direction \parencite{schilling2013walknet}.
The AEP describes the foremost point the leg reaches during it's movement cycle, the PEP the rearmost point.
While the stance phase is responsible for holding the insect up and moving it forward until reaching the PEP, the swing phase is needed to reposition the leg towards the AEP to start the cycle again \parencite{schilling2013walknet}.
The complete cycle can be described as a pushing phase (Stance) and a repositioning phase (Swing) .

Based on this elemental cycle, several distinct gait patterns emerge in nature, the most common of which are the \emph{tripod}, \emph{tetrapod} and \emph{wave} gait (different names are also common) \parencite{trotta2022walking,schilling2013walknet}.
A so called \textit{footfall pattern} can be used to visualize these gait patterns, as can be seen in \ref{figure: Footfall patterns}.
In these graphs, each row represents the state of a leg over time, the black bars indicating the swing phase.
The gait pattern an insect utilizes, loosely translates to its movement speed.
If the speed increases, the gait gradually changes from a wave gait (slowest) to a tetrapod gait and further to a tripod gait (fastest).
These transitions do not occur suddenly, but rather as a continuum \parencite{schilling2013walknet}.

\begin{figure}[h]
	\begin{subfigure}{\textwidth} % this sets the figure to be max half the width of the page
		\centering
		% include first image
		\includegraphics[scale=0.25]{TripodGait_Footfall.PNG}  % this sets the image to fill 90% of the available space -> 45% of the line width in total. 
		\caption{}
		\label{figure: tripod footfall}
	\end{subfigure}
	
	\begin{subfigure}{\textwidth}
		\centering
		% include second image
		\includegraphics[scale=0.25]{TetrapodGait_Footfall.PNG}  
		\caption{}
		\label{figure: tetrapod footfall}
	\end{subfigure}
	
	\begin{subfigure}{\textwidth}
		\centering
		% include third image
		\includegraphics[scale=0.25]{WaveGait_Footfall.PNG}   % this width should be half of the width of the other two images
		\caption{}
		\label{figure: wave footfall}
	\end{subfigure}
	\caption[Footfall Patterns]{Footfall patterns: (a) Tripod gait with a swing-stance cycle frequency of 0.5 Hz and the swing phase taking 45\% of the cycle duration (45\% duty). (b) Tetrapod gait (0.5 Hz, 33.$\overline{3}$\% duty). (c) Wave gait (0.5 Hz, 16.$\overline{6}$\% duty). The patterns were generated using the "\textit{Footfallpattern}" component created by \cite{sanandoHexapod} in Simulink during the course of his thesis.}
	\label{figure: Footfall patterns}
\end{figure}


\section{Inverse Kinematics}
Inverse Kinematics (IK) is a mathematical approach predominately used in the fields of robotics and computer graphics \parencite{AristidouFABRIK}.
It describes the process of calculating the joint angles required to place the end of a kinematic chain, the so called the end-effector, at a given position and orientation in space.
A kinematic chain serves as an abstract representation of a connected series of joints and links, such as a robotic manipulator or the arm of a human 3D model.
Essentially, IK is the opposite of the easier to solve problem of forward kinematics (FK), which is concerned with computing the position and orientation of the end-effector given a set of joint angles.
The reason behind FK being an easier problem to solve is based on the fact that, for any set of joint angles, there is only one, straightforward to compute solution \parencite{inverseKinematicsIllinois}.
The difficulty of solving IK problems arises from the fact that there can exist multiple solutions (multiple sets of joint angles) which achieve the same end-effector pose.
Some of these may be physically feasible while others might lead to collisions or instabilities.\\
There primarily exist two approaches to solving IK problems, analytical and numerical \parencite{inverseKinematicsIllinois}:

\subsection{Analytical Inverse Kinematics}
Analytical solution methods are based on trigonometric equations derived from the geometric and kinematic parameters of the manipulator arm, such as the link length, joint type and joint constraints.
We present an example of such derivations at a later point in this work, for reference see \ref{subsubsec: IK Solver}.
They provide exact solutions to the problem and only use minimal computational resources.
Although very efficient and precise, the analytical approach is generally only feasible for kinematic chains with a small number of DoF.
If the kinematic chain contains redundant DoFs, meaning it possesses more DoF than the dimension of the workspace it is in, there exist multiple or even infinite solutions to the IK problem \parencite{inverseKinematicsIllinois}.
Such systems potentially lack closed-form expressions for solutions, making it infeasible to apply analytical solution methods.

\subsection{Numerical Inverse Kinematics}
Numerical solution methods use iterate approaches to approximate the joint angles and converge towards a solution over several iterations of their algorithm.
At the start of the process, numerical methods begin with an initial guess.
This guess can be based on the manipulators geometry, joint limits, previously obtained solutions and other heuristics \parencite{inverseKinematicsIllinois}.
Using the estimated joint angles, the would-be position of the end-effector is calculated (FK) and the error between the desired pose and the currently obtained pose is determined.
Based on this, the joint angle estimates are adjusted, aiming to reduce the error and bring the end-effector closer to the desired pose.
This process is repeated until the error meets the predefined tolerances.
The joint angles calculated during the final iteration are then considered a solution to the problem \parencite{inverseKinematicsIllinois}.
Although the basic principle of iterative error-minimization is always present in numerical solution methods, the exact process of minimization is much more complex than described here and there exist numerous different techniques on how to achieve this goal, two examples being Cyclic Coordinate Descent \parencite{inverseKinematicsIllinois} and \textit{FABRIK} \parencite{AristidouFABRIK}. 
Numerical methods can be applied to a wide variety of IK problems and are not limited by the number of DoF like analytical methods.
Due to their iterative nature they are generally significantly more computationally expensive, given the same problem, than their analytical counterparts \parencite{aristidou2018inverse}.
Both IK methods are used extensively in industry and research, depending on the specific project and its requirements.


\section{PID Controller}
The \textbf{P}roportional-\textbf{I}ntegral-\textbf{D}erivative (PID) controller is the most commonly used feedback control system in modern industry \parencite{aastrom2002control}.
A feedback control system aims to regulate a control variable towards a desired setpoint by adjusting its output according to the currently measured process variable.
In most cases this process variable corresponds to a real world variable such as the speed of a motor, the liquid-level inside a tank or the temperature of a furnace.
The architecture of a PID controller consists of three terms, a proportional, integral and derivative term.
Combined, these compute the controlled output based on the error between setpoint and current process variable.
Focusing on digital PID controllers, the process of error calculation and control variable adjustment is done at a discrete, fixed rate, also referred to as a time step.
There also exist implementations of PIDs using analogue electronics which generate a continuous control signal, but these are of no concern for the content of this thesis\parencite{MATLAB_PID_playlist,TLK_Energy_PID_Explanations}.
In the following section we will describe the operation of a (digital) PID controller in detail, the information being sourced from \cite{MATLAB_PID_playlist} and \cite{TLK_Energy_PID_Explanations}:

\begin{figure}[h]
	\centerline{\includegraphics[scale=0.05]{PID_Controller}}
	\caption[PID controller illustration]{Illustration of a basic PID Controller}
	\label{figure: PID Controller}
\end{figure}

\todo{Change d and dt to /partial in pid graphic}

At each time step, the controller first calculates the error between its setpoint and the currently measured process variable: 
\[
	Error(t) = setpoint - process\ variable(t)
.\]
Following this, the controller then determines the proportional, integral and derivative term based on this error.

The proportional term $P(t)$ is calculated by multiplying the error by a constant factor, the so called proportional gain $K_p$.
This term contributes to the control output, as its name suggests, proportional to the magnitude of the error.
$P(t)$ is given by:
\[
	P(t) = K_p \cdot Error(t)
.\]
The integral term $I(t)$ takes into account the sum of past errors to address any long term bias/ constant error inside of the controlled system.
Such a bias might for example be the gravitational pull a drone experiences while it is supposed to stay at a constant height.
The term is able to eliminate long-term errors that can not be accounted for by $P(t)$.
It is calculated by integration of the error over time and multiplication with the integral gain $K_i$.
$I(t)$ is given by:
\[
	I(t) = K_i \cdot \int_{0}^{t} Error(\tau) \,d\tau
.\]
The derivative term $D(t)$ anticipates the errors future behaviour by determining the errors rate of change.
It prevents the controller from overshooting and oscillating by dampening the control response.
$D(t)$ is calculated by multiplying the errors rate of change with the constant derivative gain $K_d$.
It is given by:
\[
	D(t) = K_d \cdot \frac{\partial}{\partial t}(Error(t))
.\]
The final output of the controller at time $t$ is then given by the sum of all three terms:
\[
	Control\ Output(t) = P(t) + I(t) + D(t)
.\]
The calculated control output is fed into the controlled system or process and the PID controller begins with the calculation of the next control value.

As an additional note, in real-world applications the derivative term is almost never implemented as a pure derivative, as it would be extremely sensitive to noise.
Instead, a so called filter coefficient ($N$) is utilised in the derivative term to counteract this sensitivity.
The modified term acts as a first-order, low pass filter, attenuating high frequency noise, preventing violent swings of the derivative.
%This term acts equivalently to a derivative up to a given cutoff frequency[Hz] given by: $\frac{N}{2\pi}$ \todo{Not required}.

The performance of a PID controller greatly depends on the values of its parameters $K_p$, $K_i$ and $K_d$ (and N).
To achieve desired traits like a fast response time and minimal overshooting or oscillation, these parameters have to be carefully tuned. 
This tuning process can be performed manually via trial-and-error or methodically using a tuning algorithm.

Summarizing, a PID controller continuously repeats the process of error calculation and adjustment, forming a closed-loop system which aims to minimize the error and thus approach the setpoint as close as possible.



\section{Reinforcement Learning} \label{sec: Reinforcement Learning}
In the following section we explain the basic concepts of Reinforcement Learning (RL), focusing on the aspects relevant to understand the further content of this thesis.
We base or explanations on the works of \cite{sutton2018reinforcement} and \cite{silver2015}, citing additional sources as needed.

RL, besides Supervised and Unsupervised Learning, is one of the three major branches in the field of Machine Learning.
At its core, RL revolves around the fundamental principle of learning how to cumulatively maximize a scalar reward signal without explicit guidance on an optimal path \parencite{sutton2018reinforcement}.
On an abstract level, RL takes place as an iterative process, in which one or more agents interact with an environment by executing sequences of actions.
The experiences gained from these interactions serve as the foundation on which future actions are improved.
This ongoing process is, fittingly, called \emph{"training"} and represents the essential element of RL.
Many of the underlying principles of RL draw inspiration from behavioural psychology, where learning is predominantly driven by the consequences of one's actions \parencite{sutton2018reinforcement, joshi2021reinforcement}.
To gain a deeper understanding of RL, we will examine the three most important elements of the training process in detail:

\begin{figure}[h]
	\centerline{\includegraphics[scale=0.075]{RL_Overview}}
	\caption[RL concept illustration]{Sketch illustrating the basic concept of Reinforcement Learning}
	\label{figure: RL Illustration}
\end{figure}


The \textbf{agent}, as depicted in Fig. \ref{figure: RL Illustration}, takes the central role in the RL training process.
It receives observations $O_t$ and a reward signal $R_t$ from the environment, based on which the agent takes actions $A_t$ within the environment.
An agent always acts according to its current policy.
A policy is roughly defined as a mapping of all perceived environmental states to the action to be taken by the agent when in a given state.
It fully  describes the way a learning agent behaves at any given point in time \parencite{sutton2018reinforcement, silver2015}.
The agents policy is continually updated throughout the training process, with the overarching goal of improving the actions it prescribes.

%MATLABs \textit{Reinforcement Learning Toolbox} provides several different RL agents, some of which will will go into more detail about later.

The \textbf{environment}  can be described as the "arena", in which the agent tries to enhance its policy through taking actions and learning from the received experiences.
It defines the problem or task the RL agent is supposed to solve.
The problem space RL is applicable to is remarkably diverse, ranging from robotic control tasks such as bipedal walking or autonomous driving to playing video games and even personalized advertisement or news recommendations.
Essentially, every problem for which it is possible to define a reward function that accurately determines the agent's performance in terms of a numerical value, RL can be applied to \parencite{silver2015}.
The agent's sole way of interaction with the environment is through the actions it chooses to take.
The environment receives the actions taken and computes the resulting states and rewards.
The reward values, in addition to the observations, are provided back to the agent, guiding its learning progress.
This relationship between agent and environment forms the essence of RL, every action resulting in a reaction, based on which the policy can be optimized.


\begin{comment}
%Discrete vs. Continuous action space in RL learning: 
Using the RL toolbox, 2 distinct types of environment are provided which differ in the definition of their action space.
If an environment has a discrete action space, it means that there exists countable, discrete actions which can be taken.
Discrete action spaces are used when the number of possible actions is limited and known in advance.
An example of such an environment would be a game of chess; for each turn there is a finite number of available moves.
Continuous action spaces on the other hand have a continuum of actions which can be taken and are used when the number of possible actions is infinite, such as the movement possibilities of a robotic arm.	
\end{comment}

The \textbf{reward function} or reward signal plays an important role in shaping the behaviour of an agent.
It is a scalar function which serves as a measure of agents performance given a predefined goal.
At each time step in the environment, the function is evaluated, providing a scalar output known as the reward.
The reward serves as feedback to the agent, representing the desirability of the chosen actions in the current state.
%The maximization of this reward is the sole objective of the agent \parencite{silver2015}.
Due to the reward function being the only metric based on which the agents behaviour is judged, it's specification is of immense importance.
A poorly defined reward function will almost certainly result in a poorly performing agent.
It is noteworthy that the reward function is considered to be part of the environment, but is not a dynamic component that changes under the agents actions.
Instead, it is a fixed criterion for the agents performance.
\parencite{sutton2018reinforcement}

A term often coined in the field of RL is the \text{Markov Decision Process} (MDP).
It defines a mathematical framework for decision making in situations where the outcomes of decisions are partially random.
As RL fits into this framework well, MDPs can provide a useful theoretical foundation.\\

\begin{definition}[Markov Decision Process {[\cite{silver2015}]}]
	A \textbf{MDP} is a 5-tuple $\langle \mathcal{S,A,P,R,\gamma} \rangle$
	\begin{itemize}
		\item $\mathcal{S}$ is a finite set of states
		\item $\mathcal{A}$ is a finite set of actions
		\item $\mathcal{P}$ is a state transition probability matrix, $\mathcal{P}_{ss'}^a = \mathbb{P}[S_{t+1} = s' \ | \ S_t = s, A_t = a	] $
		\item $\mathcal{R}$ is a reward function, $\mathcal{R}_s^a = \mathbb{E}[R_{t+1} \ | \ S_t = s, A_t = a]$
		\item $\mathcal{\gamma}$ is a discount factor $\mathcal{\gamma} \in [0,1]$
	\end{itemize}
\end{definition}

In the context of RL, the elements of a MDP take on the following meanings:\\
$\mathcal{S}$ is the set of all states the environment can be in, $\mathcal{A}$ the set of all actions the agent can decide to take.
$\mathcal{P}$ provides the probability of the environment transitioning to state $s'$, given it is in state $s$ and the agent takes action $a$ ($\forall s \in \mathcal{S}, a \in \mathcal{A}$).
$\mathcal{R}$ represents a reward function in the same sense as already explained above.
Lastly, $\mathcal{\gamma}$ discounts the value of future reward due to it's uncertainty.
More exactly: The value of  reward r after k+1 steps is given as $r\mathcal{\gamma}^k$.

There exist various techniques for policy optimization.
In MATLAB, the \emph{Reinforcement Learning} toolbox offers several different RL algorithms to choose from, including DDPG, TR3, PPO, SAC and TRPO.
In the upcoming paragraph, a concentrated overview of the algorithms utilized in this thesis is given, namely DDPG (Deep Deterministic Policy Gradient) and PPO (Proximal Policy Optimization).
This overview is based on the work of \cite{silver2015}, as well as MATLABs RL documentation and OpenAIs \textit{Spinning Up} documentation \parencite{matlabRLDocumentation, openAI_SpinningUp_Documentation}.
Given the size limitations imposed on this thesis, focus lies on only the most essential elements.

Policy Gradient (PG) methods in general are a class of RL techniques focusing on directly optimizing an agent's policy (instead of e.g. trying to learn a model of the environment or state values).
Simplified, a PG indicates the direction in which a policy has to be changed, to achieve the steepest increase in cumulative reward.
In real applications this PG is often approximated, as a deterministic calculation would be infeasible in most cases.\\
\textbf{DDPG}, combining elements from deep learning and PG methods, employs a so called actor-critic architecture, consisting of two neural networks (NN).
The actor is responsible for learning a deterministic policy, typically implemented as a deep NN (DNN).
The critic, a DNN as well, evaluates the actions chosen by the actor. It estimates the expected cumulative reward (Q-value) of taking a specific action in a given state.
This evaluation guides the actor towards better actions.\\
To introduce noise into the actions taken during training, DDPG utilizes the so called Ornstein-Uhlenbeck(OU) noise model, which generates a stochastic variable randomly fluctuating around a mean value.
The introduced noise promotes exploration, which is crucial for overcoming local optima during the learning process \parencite{openAI_DDPG}.\\
As a PG method as well, \textbf{PPO} employs the actor-critic architecture to train a NN on finding an optimal policy.
The main innovation PPO introduces, is a limit on the size of policy updates, preventing a policy from changing to rapidly during training.
The policy always stays in "proximity" to its previous version, promoting learning stability and preventing sharp performance collapses \parencite{openAI_PPO}.\\


\begin{comment}

\parencite{weng2018bandit}
\parencite{sutton2018reinforcement}

Model-Free vs. Model ?


\begin{definition*}
	A policy $\pi$ is a distribution over actions given states,\\
	$\pi(a|s) = \mathbb{P}(A\textsubscript{t} = a\,|\,S\textsubscript{t} = s)$
\end{definition*}
A policy fully defines an agents behavior. 
\todo{Cite David Sijlver Lec. 2}

A value function, in contrast to a reward function which immediately rewards good actions, defines what is good long term. 
Rewards only define the immediate desirability of environmental states, a value function takes into account states which are likely to follow a given state and the rewards available in those states.
A state might immediately yield a low reward, but if it is likely followed by states which yield high rewards, it can still possess a high value.
This can also be true for the opposite, a state has a high immediate reward, but is likely only followed by states which yield low rewards. Thus the state has a low value \parencite{sutton2018reinforcement}.

Some methods of solving RL problems also consider a model of the environment. This means that the approach has a way of planning ahead, such as predicting the next environmental state and reward.
Methods which use a model are called model-based, methods which explicitly only learn by trial and error model-free\parencite{sutton2018reinforcement}.

Offline vs. Online Reinforcement Learning: 
RL agents labeled as offline only learn from a fixed dataset which has been acquired before starting the training process.
The agent itself does not interact with the environment, only learning on historical data.
Online RL agents on the other side learn while actively interacting with the environment.
They decide, act and receive feedback all in real-time and learn from the consequences of their actions \parencite{schrittwieser2021online}.


Markov Property: "The future is independent of the past given the present"
The current state contains all relevant information to make predictions about the future
\begin{definition*}
	State S\textsubscript{t} is \textbf{\textit{Markov}}, if and only if
	$ P(S\textsubscript{t+1} | S\textsubscript{t}) = P(S\textsubscript{t+1} | S\textsubscript{1},...,S\textsubscript{t}) $.
\end{definition*}

Markov Process:
A Markov Process is a memory-less random process, i.e. a sequence of random states S\textsubscript{1}, S\textsubscript{2},... that have the Markov property.

\begin{definition*}
	A Markov Process (or Markov Chain) is a tuple $\langle\mathcal{S,P}\rangle$ where
		\begin{itemize}
		\item $\mathcal{S}$ is a (finite) set of states that have the Markov property
		\item $\mathcal{P}$ is a state-transition probability matrix,\\
		$\mathcal{P}\textsubscript{ss'} = \mathbb{P}[S\textsubscript{t+1} = s'|S\textsubscript{t}=s] $
	\end{itemize}
\end{definition*}
\todo{Cite David Silver RL course Lec. 1-2 for everything about Markov}

Markov Decision Process(MDP):

$\rightarrow$ Hexapod locomotion can be considered a MDP, see: \parencite{ouyang2021adaptive} [p.7, 4.1]
$\rightarrow$ Try DDPG for RL, allegedly widely used in robotics \parencite{ouyang2021adaptive}


Agents applicable for continuous action spaces: 
\begin{itemize}		
	\item DDPG: Deep Deterministic Policy Gradient
	Model-free, continuous action space, actor-critic (Look at \parencite{trotta2022walking})
	\item TD3: Twin-Delayed Deep Deterministic Policy Gradient (more complex improvement to DDPG)
	Model-free, continuous action space, actor-critic
	\item PPO: Proximal Policy Optimization (more stable updates, but longer training)
	Model-free, continuous or discrete action space; policy gradient rl-method
	\item SAC: Soft Actor-Critic (more complex improvement of DDPG generating stochastic policies)
	Model-free, continuous action space, actor-critic
	\item TRPO: Trust Region Policy Optimization(more complex version of PPO, more robust for deterministic environments with fewer iterations)
\end{itemize}
\end{comment}